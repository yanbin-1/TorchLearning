{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    '''\n",
    "    构建相同的卷积层\n",
    "\n",
    "    num_convs: 卷积层的数量\n",
    "    in_channels: 输入通道数\n",
    "    out_channels: 输出通道数\n",
    "    '''\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # 全连接层部分\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "# 加载Fashion-MNIST数据集\n",
    "def loadFashion(root, trans, batch_size, resize=None, download=False):\n",
    "    if resize:\n",
    "        trans.insert(0, torchvision.transforms.Resize(size=resize))\n",
    "    trans = torchvision.transforms.Compose(trans)\n",
    "\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root=root, train=True, transform=trans, download=download)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root=root, train=False, transform=trans, download=download)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval()\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).sum().item()\n",
    "            # 改回训练模式\n",
    "            net.train()\n",
    "\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, optimizer, loss, num_epochs, device):\n",
    "    net = net.to(device)\n",
    "    print('training on', device)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count = 0, 0, 0, 0\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            y_hat = net(X)\n",
    "            print(y_hat.shape, y.shape)\n",
    "            print(y_hat, y)\n",
    "            l = loss(y, y_hat)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l += l.items()\n",
    "            train_acc += (y_hat.argmax(dim=1) == y).sum().items()\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\"epoch %d, loss %.4f, train acc %.3f, test acc %.3f\" % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "torch.Size([64, 10]) torch.Size([64])\n",
      "tensor([[-6.6448e-03,  5.3274e-03, -6.7170e-03, -7.9008e-03, -2.9880e-03,\n",
      "          3.7116e-03, -3.4850e-03,  1.2013e-02,  5.4014e-03,  1.9229e-02],\n",
      "        [-8.4889e-03, -3.2992e-03,  1.3547e-03, -1.4991e-02, -2.1871e-03,\n",
      "          1.1027e-02, -1.5944e-02,  2.2126e-02,  1.4447e-02,  9.5951e-03],\n",
      "        [-1.0463e-02, -6.8581e-03, -5.7257e-03, -2.1024e-04, -6.8161e-05,\n",
      "          1.4949e-02, -5.1320e-03,  1.5571e-02,  1.2104e-02,  2.2553e-02],\n",
      "        [-7.4170e-03,  8.9638e-03, -3.7648e-03, -1.5423e-02,  7.6153e-03,\n",
      "          1.6581e-02,  7.2702e-03,  6.9583e-03,  1.0425e-02,  1.4100e-02],\n",
      "        [ 4.8519e-04, -7.0686e-04, -6.0318e-03, -1.0426e-02,  1.1282e-02,\n",
      "          1.5330e-02, -2.9496e-03,  1.0973e-02,  6.5666e-03,  2.1552e-02],\n",
      "        [-9.2656e-03, -1.8314e-03, -1.4939e-03, -3.5001e-03,  3.2234e-03,\n",
      "          1.3688e-02,  1.0670e-03,  2.3732e-03,  8.4491e-03,  8.9327e-03],\n",
      "        [-4.5618e-03, -4.6092e-03, -8.2390e-03, -1.4093e-02,  7.1129e-03,\n",
      "          9.9014e-03, -5.7051e-03,  4.4865e-03, -1.4401e-03,  1.7229e-02],\n",
      "        [-4.6971e-03, -4.0715e-04, -9.8892e-03, -8.9055e-03,  9.1776e-03,\n",
      "          4.2932e-03, -9.9320e-03,  1.3678e-02,  1.5475e-02,  1.6313e-02],\n",
      "        [-7.9667e-03,  2.2637e-03, -1.0545e-02, -6.8168e-04,  3.2528e-03,\n",
      "          1.0555e-02, -4.5287e-03,  9.8978e-03,  6.0001e-03,  1.8696e-02],\n",
      "        [-1.0882e-02,  8.2723e-03, -5.0747e-03, -1.0313e-02,  1.3951e-03,\n",
      "          1.4266e-02, -8.5001e-04,  8.9156e-03,  1.4430e-02,  1.7621e-02],\n",
      "        [-5.8093e-03,  1.2476e-02, -5.7727e-03, -1.2809e-02,  1.1634e-03,\n",
      "          1.5165e-02, -2.6355e-03,  1.1476e-02,  1.2499e-02,  2.6490e-02],\n",
      "        [-1.8755e-02,  8.3346e-03, -4.1077e-03, -1.1379e-02, -3.1369e-04,\n",
      "          1.0254e-02, -2.8873e-03,  8.7011e-03,  4.9681e-03,  1.9484e-02],\n",
      "        [-3.7060e-03,  6.1528e-03, -7.4284e-03, -9.2306e-04, -2.3499e-03,\n",
      "          6.2821e-03,  5.6756e-03,  1.5583e-02,  7.9526e-03,  1.4647e-02],\n",
      "        [-4.2302e-03,  5.7771e-03,  1.7311e-03, -9.1997e-05,  4.4767e-03,\n",
      "          3.5162e-03,  4.4707e-03,  1.0545e-02,  1.6919e-02,  1.6927e-02],\n",
      "        [-6.8810e-03,  7.7600e-03, -3.7931e-03, -7.0088e-03,  1.7626e-03,\n",
      "          1.0678e-02, -1.0400e-02,  1.4734e-02,  1.2845e-02,  2.3856e-02],\n",
      "        [-1.1867e-02, -1.6379e-03, -3.0213e-04, -8.7515e-03,  5.1046e-03,\n",
      "          6.2573e-03,  1.9977e-03,  1.5064e-02,  8.8188e-03,  1.2775e-02],\n",
      "        [ 2.3066e-03,  7.0324e-03, -3.2175e-04, -9.3486e-03,  1.2323e-02,\n",
      "          1.1327e-02,  1.5645e-03,  1.3556e-02,  6.4154e-03,  1.5377e-02],\n",
      "        [-1.4853e-03,  6.4784e-03,  2.9617e-04, -1.3728e-02,  5.7811e-03,\n",
      "          3.6628e-03, -1.8411e-03,  1.3488e-02,  1.6253e-02,  2.1845e-03],\n",
      "        [-9.3115e-03,  1.2573e-02, -7.8886e-03, -3.7213e-03,  5.2968e-03,\n",
      "          8.5711e-03, -3.2901e-03,  5.9303e-03,  1.7271e-02,  2.3690e-02],\n",
      "        [-1.2165e-02,  1.4439e-03, -5.7905e-04, -4.3575e-03,  4.1055e-03,\n",
      "          1.0212e-02, -7.5175e-03,  1.1631e-02,  1.0948e-02,  1.7167e-02],\n",
      "        [-4.8028e-03,  6.2333e-03, -6.8718e-03, -1.4615e-02,  7.2667e-03,\n",
      "          4.5263e-03, -5.2506e-03,  1.7304e-02,  6.6104e-03,  2.9452e-02],\n",
      "        [-6.9056e-03,  1.8685e-03, -1.4849e-03, -5.7210e-03,  3.6341e-03,\n",
      "          1.1541e-02, -4.6871e-03,  9.0981e-03,  1.3449e-03,  1.7408e-02],\n",
      "        [-1.2746e-02, -5.0401e-03, -5.7858e-03, -2.1779e-02,  4.4538e-03,\n",
      "          9.2972e-03, -1.9406e-03,  1.0135e-02,  9.7421e-03,  1.3240e-02],\n",
      "        [-1.5837e-03,  1.0880e-02, -2.4160e-03, -7.3919e-03,  4.5669e-04,\n",
      "          9.2372e-03, -8.3460e-03,  1.5778e-02,  1.4656e-03,  2.2381e-02],\n",
      "        [-4.4017e-03,  4.8126e-03, -1.1021e-02, -8.3992e-03,  5.2200e-03,\n",
      "          1.1841e-02, -1.0648e-03,  1.5097e-02,  3.5436e-03,  1.1006e-02],\n",
      "        [-9.5650e-03,  1.7081e-03,  3.6257e-03, -1.1521e-02,  5.4072e-03,\n",
      "          1.4862e-02,  3.0795e-03,  1.4445e-02,  1.7358e-02,  1.7876e-02],\n",
      "        [-1.5353e-03,  4.6885e-03,  4.4835e-03, -9.8362e-03, -3.7629e-03,\n",
      "          9.6873e-03, -4.1922e-03,  1.0978e-02,  5.9047e-03,  2.0029e-02],\n",
      "        [-1.4018e-02,  8.3900e-03,  2.0759e-03, -8.7554e-04,  9.2649e-03,\n",
      "          7.3434e-03, -1.1829e-02,  1.0303e-02,  9.1809e-03,  7.3934e-03],\n",
      "        [-1.1943e-02,  5.9569e-03, -3.3004e-03, -1.1021e-02,  2.2580e-03,\n",
      "          8.9640e-03, -5.5228e-03,  1.0623e-02,  2.2252e-02,  1.5905e-02],\n",
      "        [-1.3107e-03,  1.1341e-02, -3.5680e-03, -2.0090e-02,  1.1424e-03,\n",
      "          2.7059e-03, -6.3438e-03,  1.4267e-02,  1.4194e-02,  1.7386e-02],\n",
      "        [ 1.7124e-03,  3.1727e-03, -1.2492e-03, -1.0531e-02,  4.4742e-03,\n",
      "          5.3075e-03, -2.4593e-03,  1.5871e-02,  4.7928e-03,  1.5654e-02],\n",
      "        [-3.8385e-03,  4.7746e-03,  4.5456e-03, -1.5609e-02,  4.5265e-03,\n",
      "          5.1575e-03, -1.1409e-02,  1.0526e-02,  9.2132e-03,  1.4972e-02],\n",
      "        [-1.2177e-02,  9.1308e-03, -5.7580e-03, -6.8973e-03,  1.3398e-02,\n",
      "          1.5762e-02, -9.4803e-03,  1.3038e-02,  5.1791e-03,  1.7670e-02],\n",
      "        [-5.7379e-03,  8.3617e-03, -8.3314e-03, -1.2123e-02,  3.3381e-03,\n",
      "          1.3849e-02, -9.3501e-03,  1.4937e-02,  6.0233e-03,  1.0434e-02],\n",
      "        [-3.2781e-04,  4.7011e-03, -9.0610e-04, -4.1184e-03, -2.6495e-03,\n",
      "          3.0555e-03, -2.4416e-03,  9.0228e-03,  1.6020e-03,  1.2360e-02],\n",
      "        [ 1.4281e-03,  5.6428e-03, -1.0841e-03, -1.1145e-02,  2.3607e-03,\n",
      "         -4.3924e-03, -7.5869e-03,  1.0785e-02,  1.8651e-02,  2.2440e-02],\n",
      "        [ 2.7575e-04,  5.2313e-03, -1.9962e-04, -1.5621e-02,  1.5674e-03,\n",
      "          8.4799e-03, -5.7352e-04,  1.8300e-02,  3.8169e-03,  1.1689e-02],\n",
      "        [-7.9414e-03,  1.2704e-02, -3.5417e-03, -9.9554e-03, -2.4996e-03,\n",
      "          1.1316e-02,  1.2995e-03,  1.3048e-02,  1.8406e-02,  1.8737e-02],\n",
      "        [-1.0102e-02, -3.4325e-03, -7.4278e-03, -5.7647e-03, -6.4744e-03,\n",
      "          6.9929e-03, -5.5561e-03,  1.5832e-02,  4.9787e-03,  1.5837e-02],\n",
      "        [-4.3327e-03,  1.7833e-04,  2.2964e-03, -3.1553e-03,  2.0530e-03,\n",
      "          1.1036e-02, -4.5808e-03,  1.8607e-02,  1.5281e-02,  2.3164e-02],\n",
      "        [-1.2189e-02,  9.1726e-03, -9.8432e-03, -1.0354e-02,  9.7048e-03,\n",
      "          4.1969e-03,  4.7499e-03,  1.1166e-02,  1.0707e-02,  2.5405e-02],\n",
      "        [-6.6881e-03, -4.7340e-03, -1.3451e-02, -1.2641e-02,  6.6875e-03,\n",
      "          1.7644e-02, -4.2901e-03,  1.1829e-02,  1.2084e-02,  3.1387e-02],\n",
      "        [ 1.1645e-03,  2.5846e-04, -8.0221e-03, -8.2090e-03,  8.2773e-03,\n",
      "          1.1943e-02, -4.7940e-03,  1.4661e-02,  9.0283e-04,  1.1777e-02],\n",
      "        [-1.1195e-02,  1.2438e-02, -1.2525e-03, -6.5869e-03,  6.6275e-03,\n",
      "          6.0328e-03, -5.9622e-03,  1.1243e-02,  5.1079e-03,  2.0981e-02],\n",
      "        [-7.2468e-03,  4.4944e-03, -1.0956e-03, -4.9481e-04,  3.6843e-03,\n",
      "          1.3051e-02, -5.9877e-03,  8.6334e-03,  3.8906e-03,  2.4886e-02],\n",
      "        [-4.6223e-03,  3.9277e-03,  2.4672e-04, -5.3323e-03, -9.5713e-04,\n",
      "          6.5348e-03, -1.3378e-02,  3.4817e-03, -3.3278e-04,  1.2267e-02],\n",
      "        [-1.3036e-03, -9.7698e-04, -1.1740e-03, -1.3551e-02,  8.0586e-03,\n",
      "          1.0669e-02,  2.2376e-03,  8.7628e-03,  8.8314e-03,  1.4033e-02],\n",
      "        [-5.4559e-03,  5.6353e-03, -5.3746e-03, -7.2540e-03,  2.4875e-03,\n",
      "          1.1221e-02, -2.0010e-03,  8.0281e-03,  1.6240e-02,  2.4131e-02],\n",
      "        [-7.8797e-03, -1.7862e-03,  3.6015e-03, -1.0881e-02,  7.9683e-03,\n",
      "          1.4109e-02, -7.8211e-03,  1.6079e-02,  1.8424e-03,  7.7718e-03],\n",
      "        [-1.0040e-02,  6.8880e-03, -4.1347e-03, -3.1798e-03,  7.3951e-03,\n",
      "          4.9178e-03,  1.7108e-03,  1.5568e-02,  1.0949e-02,  3.1387e-02],\n",
      "        [-4.3417e-03,  7.6813e-03, -7.7607e-03, -4.0108e-03,  8.8813e-04,\n",
      "          1.1592e-02, -7.7452e-03,  1.2216e-02,  6.9907e-03,  1.7465e-02],\n",
      "        [-3.4849e-04,  8.1530e-03, -6.5330e-03, -1.3016e-02, -1.7148e-03,\n",
      "          1.7842e-02, -1.2990e-02,  6.0397e-03,  9.9191e-03,  1.2565e-02],\n",
      "        [-1.2208e-04,  8.2622e-03,  4.1810e-03, -4.4567e-03, -5.7009e-03,\n",
      "          1.1934e-02, -6.1805e-03,  8.5513e-03,  6.1035e-03,  1.8169e-02],\n",
      "        [-7.9469e-03,  9.1942e-03,  3.6775e-03, -5.6543e-04,  1.4234e-03,\n",
      "          1.6031e-02, -4.2063e-03,  1.2608e-02,  2.1662e-02,  1.2128e-02],\n",
      "        [-1.4841e-02,  1.0503e-02,  2.0053e-04, -1.1631e-02, -8.6984e-04,\n",
      "          1.2721e-02,  1.0697e-03,  1.6580e-02,  1.2243e-02,  7.5229e-03],\n",
      "        [-5.8876e-03,  2.1277e-02, -2.5425e-03, -7.8250e-03,  2.0233e-03,\n",
      "          1.4202e-02, -5.7269e-03,  1.4679e-02,  2.8218e-03,  2.1559e-02],\n",
      "        [-7.2565e-03, -2.8833e-03,  2.7806e-03, -3.3717e-03,  8.0212e-03,\n",
      "          1.2973e-02, -1.3048e-02,  1.1941e-02,  1.8622e-02,  2.3065e-02],\n",
      "        [-4.6740e-03,  6.4695e-03, -3.9841e-03, -9.8652e-03, -1.7543e-03,\n",
      "          2.7757e-03, -5.4885e-03,  1.2201e-02,  1.3499e-02,  2.6924e-02],\n",
      "        [-2.0835e-03,  5.9251e-04, -6.5454e-03, -1.0659e-02,  3.5007e-03,\n",
      "          1.7202e-02, -9.8774e-04,  1.5935e-02,  4.0059e-03,  1.7284e-02],\n",
      "        [-3.1671e-03,  6.2863e-03,  4.9192e-05, -2.0518e-02,  7.9793e-03,\n",
      "          1.4633e-02, -1.5449e-02,  1.9482e-02,  9.4283e-03,  1.4480e-02],\n",
      "        [-1.6533e-02,  1.7218e-03, -3.6969e-03,  8.9416e-06, -1.4625e-03,\n",
      "          1.3678e-02, -5.2921e-03,  8.7192e-03,  3.7587e-03,  2.1848e-02],\n",
      "        [-3.3686e-03,  5.5944e-03,  1.8408e-03, -1.5100e-02,  2.4744e-03,\n",
      "          1.2406e-02, -4.3394e-03,  2.0278e-02,  1.2473e-02,  2.4202e-02],\n",
      "        [-5.4664e-03,  6.6159e-03,  3.9570e-03, -1.2871e-02,  8.1883e-03,\n",
      "          1.0065e-02, -1.1006e-03,  2.8425e-03,  5.8481e-03,  2.3046e-02],\n",
      "        [-8.1717e-03,  4.5341e-04, -2.1757e-03, -1.0543e-02, -6.9648e-03,\n",
      "          4.6276e-03,  3.4871e-03,  4.5101e-03,  5.5515e-04,  2.4340e-02]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>) tensor([8., 6., 3., 5., 4., 8., 8., 1., 6., 1., 9., 6., 2., 9., 4., 6., 0., 2.,\n",
      "        6., 7., 6., 4., 8., 4., 5., 2., 3., 4., 4., 5., 6., 5., 0., 9., 2., 1.,\n",
      "        8., 6., 6., 1., 2., 0., 3., 5., 4., 2., 5., 6., 5., 7., 3., 2., 2., 1.,\n",
      "        5., 2., 7., 4., 3., 5., 7., 1., 4., 2.], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     17\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m train(net, train_iter, test_iter, optimizer, loss, num_epochs, device)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, optimizer, loss, num_epochs, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(y_hat\u001b[39m.\u001b[39mshape, y\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(y_hat, y)\n\u001b[1;32m---> 12\u001b[0m l \u001b[39m=\u001b[39m loss(y, y_hat)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m l\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1213\u001b[0m, in \u001b[0;36mMultiLabelSoftMarginLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1213\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmultilabel_soft_margin_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\functional.py:3428\u001b[0m, in \u001b[0;36mmultilabel_soft_margin_loss\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3425\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3426\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3428\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(target \u001b[39m*\u001b[39;49m logsigmoid(\u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m target) \u001b[39m*\u001b[39m logsigmoid(\u001b[39m-\u001b[39m\u001b[39minput\u001b[39m))\n\u001b[0;32m   3430\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3431\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m*\u001b[39m weight\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 缩减VGG网络通道数\n",
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)\n",
    "\n",
    "batch_size = 64\n",
    "trans = [torchvision.transforms.ToTensor()]\n",
    "train_iter, test_iter = loadFashion(root=\"../../Data/FashionMNIST\", trans=trans, batch_size=batch_size, resize=224, download=False)\n",
    "\n",
    "lr = 0.05\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "loss = torch.nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train(net, train_iter, test_iter, optimizer, loss, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 9],\n",
      "        [6, 3, 6]]) tensor([[5],\n",
      "        [0]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m, (\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(a, b)\n\u001b[1;32m----> 6\u001b[0m loss(a, b)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "a = torch.randint(0, 10, (2, 3))\n",
    "b = torch.randint(0, 10, (2, 1))\n",
    "print(a, b)\n",
    "loss(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
