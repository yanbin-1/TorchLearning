{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67fd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223942ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真实方程如下"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9fa0b82a",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f4899",
   "metadata": {},
   "source": [
    "## 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9916efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 20  # 多项式的最大阶数（这里直接取4不好吗）\n",
    "n_train, n_test = 100, 100  # 训练和测试数据集大小\n",
    "true_w = np.zeros(max_degree)  # 分配大量的空间\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
    "for i in range(max_degree):\n",
    "    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!\n",
    "# labels的维度:(n_train+n_test,)\n",
    "labels = np.dot(poly_features, true_w).reshape((n_train + n_test, 1))  # 点乘\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027cca7",
   "metadata": {},
   "source": [
    "## 数据集制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4064b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadArray(features, labels, batch_size, is_train=True):\n",
    "    dataset = torch.utils.data.TensorDataset(features, labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29198d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = min(10, poly_features.shape[0])\n",
    "\n",
    "# NumPy ndarray转换为tensor\n",
    "true_w, features, poly_features, labels = [torch.tensor(x, dtype=\n",
    "    torch.float32) for x in [true_w, features, poly_features, labels]]\n",
    "\n",
    "train_loader = loadArray(poly_features[:n_train, :4], labels[:n_train], batch_size)\n",
    "test_loader = loadArray(poly_features[n_train:, :4], labels[n_train:], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbab5d8",
   "metadata": {},
   "source": [
    "## 网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab37061",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num = 4\n",
    "output_num = 1\n",
    "# 不设置偏置，因为我们已经在多项式中实现了它\n",
    "# poly_features * true_w = labels，不设置偏置，线性层的权重正好为true_w\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_num, output_num, bias=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6994d7b",
   "metadata": {},
   "source": [
    "## 损失函数 & 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caaec139",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "loss = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59728d3",
   "metadata": {},
   "source": [
    "## 数据统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f07dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pre, y):\n",
    "    \"\"\"训练集准确率，应用于分类问题\"\"\"\n",
    "    if len(y_pre.shape) > 1 and y_pre.shape[1] > 1:\n",
    "        y_pre = torch.argmax(y_pre, axis=1)  # 分类问题\n",
    "    cmp = (y_pre == y)\n",
    "    return float(cmp.sum())\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def evaluate_test(net, data_loader, loss):\n",
    "    \"\"\"测试数据集上进行评估\"\"\"\n",
    "    metric = Accumulator(2)  # 测试数据集上的损失，准确率\n",
    "    for X, y in data_loader:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out, y)\n",
    "        metric.add(float(l.sum()), accuracy(out, y))\n",
    "    return metric[0] / n_test, metric[1] / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0756db3",
   "metadata": {},
   "source": [
    "## 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834ef0f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, loss, optimizer):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.train()\n",
    "    metric = Accumulator(2)  # 后面要统计训练集损失，训练集准确率\n",
    "    for X, y in train_loader:\n",
    "        y_pre = net(X)\n",
    "        l = loss(y_pre, y)\n",
    "        if isinstance(optimizer, torch.optim.Optimizer):\n",
    "            # Using PyTorch in-built optimizer & loss criterion\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        metric.add(float(l.sum()), accuracy(y_pre, y))\n",
    "    return metric[0] / n_train, metric[1] / n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738396b",
   "metadata": {},
   "source": [
    "### 正常拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15f8f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train_loss:  2.55821102142334\n",
      "1 train_loss:  1.8449166107177735\n",
      "2 train_loss:  1.3790075588226318\n",
      "3 train_loss:  1.0689416646957397\n",
      "4 train_loss:  0.8554395723342896\n",
      "5 train_loss:  0.7056321239471436\n",
      "6 train_loss:  0.6025066542625427\n",
      "7 train_loss:  0.5185874474048614\n",
      "8 train_loss:  0.45808829545974733\n",
      "9 train_loss:  0.40898084282875063\n",
      "10 train_loss:  0.3698444664478302\n",
      "11 train_loss:  0.3364946985244751\n",
      "12 train_loss:  0.30706611037254333\n",
      "13 train_loss:  0.2838097071647644\n",
      "14 train_loss:  0.260036832690239\n",
      "15 train_loss:  0.2409714639186859\n",
      "16 train_loss:  0.22407012045383454\n",
      "17 train_loss:  0.2089025545120239\n",
      "18 train_loss:  0.19373068273067473\n",
      "19 train_loss:  0.18050798833370207\n",
      "19 test_loss:  0.3305789905786514\n",
      "20 train_loss:  0.1693505871295929\n",
      "21 train_loss:  0.15809400081634523\n",
      "22 train_loss:  0.14843464136123657\n",
      "23 train_loss:  0.14075377762317656\n",
      "24 train_loss:  0.13111480444669724\n",
      "25 train_loss:  0.12367036342620849\n",
      "26 train_loss:  0.11633597880601883\n",
      "27 train_loss:  0.10991377025842666\n",
      "28 train_loss:  0.10406007409095765\n",
      "29 train_loss:  0.09846695989370347\n",
      "30 train_loss:  0.09405220836400986\n",
      "31 train_loss:  0.08868708282709122\n",
      "32 train_loss:  0.08484280854463577\n",
      "33 train_loss:  0.08130288660526276\n",
      "34 train_loss:  0.07719088673591613\n",
      "35 train_loss:  0.07371045678853988\n",
      "36 train_loss:  0.07064603745937348\n",
      "37 train_loss:  0.06747035756707191\n",
      "38 train_loss:  0.06485999822616577\n",
      "39 train_loss:  0.06227553814649582\n",
      "39 test_loss:  0.3875099378824234\n",
      "40 train_loss:  0.06005688160657883\n",
      "41 train_loss:  0.057642245888710024\n",
      "42 train_loss:  0.055777963697910306\n",
      "43 train_loss:  0.053693202286958695\n",
      "44 train_loss:  0.05197498083114624\n",
      "45 train_loss:  0.050246672332286836\n",
      "46 train_loss:  0.04865074619650841\n",
      "47 train_loss:  0.047386116087436675\n",
      "48 train_loss:  0.04567901164293289\n",
      "49 train_loss:  0.04446198046207428\n",
      "50 train_loss:  0.04320235073566437\n",
      "51 train_loss:  0.04198323369026184\n",
      "52 train_loss:  0.04094523847103119\n",
      "53 train_loss:  0.03974063515663147\n",
      "54 train_loss:  0.03864228770136833\n",
      "55 train_loss:  0.0376577964425087\n",
      "56 train_loss:  0.036795368641614916\n",
      "57 train_loss:  0.03581861957907677\n",
      "58 train_loss:  0.03523516222834587\n",
      "59 train_loss:  0.0342590094357729\n",
      "59 test_loss:  0.3590638330578804\n",
      "60 train_loss:  0.033361605256795886\n",
      "61 train_loss:  0.03266599602997303\n",
      "62 train_loss:  0.03201212853193283\n",
      "63 train_loss:  0.03134508430957794\n",
      "64 train_loss:  0.030587671399116514\n",
      "65 train_loss:  0.02989553064107895\n",
      "66 train_loss:  0.029361599683761598\n",
      "67 train_loss:  0.02873121149837971\n",
      "68 train_loss:  0.028195379376411436\n",
      "69 train_loss:  0.027566916793584823\n",
      "70 train_loss:  0.027022500336170197\n",
      "71 train_loss:  0.026467460170388223\n",
      "72 train_loss:  0.0260580775141716\n",
      "73 train_loss:  0.025465851798653603\n",
      "74 train_loss:  0.024843604788184166\n",
      "75 train_loss:  0.024453026950359346\n",
      "76 train_loss:  0.023956726677715777\n",
      "77 train_loss:  0.02352409265935421\n",
      "78 train_loss:  0.02323320206254721\n",
      "79 train_loss:  0.022624046690762043\n",
      "79 test_loss:  0.28447333179414275\n",
      "80 train_loss:  0.02225867599248886\n",
      "81 train_loss:  0.021808840557932852\n",
      "82 train_loss:  0.02140308830887079\n",
      "83 train_loss:  0.020991601943969727\n",
      "84 train_loss:  0.02061820328235626\n",
      "85 train_loss:  0.02026555858552456\n",
      "86 train_loss:  0.019915881901979446\n",
      "87 train_loss:  0.019537190049886702\n",
      "88 train_loss:  0.01920871131122112\n",
      "89 train_loss:  0.01887778878211975\n",
      "90 train_loss:  0.018491303995251656\n",
      "91 train_loss:  0.018181259892880917\n",
      "92 train_loss:  0.017850617840886117\n",
      "93 train_loss:  0.017479259967803955\n",
      "94 train_loss:  0.017283549606800078\n",
      "95 train_loss:  0.016970978863537312\n",
      "96 train_loss:  0.016650260090827942\n",
      "97 train_loss:  0.016430092267692088\n",
      "98 train_loss:  0.016030599139630796\n",
      "99 train_loss:  0.015718654543161393\n",
      "99 test_loss:  0.2105281361937523\n",
      "100 train_loss:  0.015469721630215644\n",
      "101 train_loss:  0.015313805565237999\n",
      "102 train_loss:  0.014935803264379502\n",
      "103 train_loss:  0.014707004092633725\n",
      "104 train_loss:  0.014450798965990544\n",
      "105 train_loss:  0.014208132661879062\n",
      "106 train_loss:  0.013972722589969636\n",
      "107 train_loss:  0.013803727999329567\n",
      "108 train_loss:  0.01348313882946968\n",
      "109 train_loss:  0.013224753104150295\n",
      "110 train_loss:  0.012999328970909118\n",
      "111 train_loss:  0.01281327821314335\n",
      "112 train_loss:  0.012642262764275073\n",
      "113 train_loss:  0.012370331920683384\n",
      "114 train_loss:  0.012193733602762222\n",
      "115 train_loss:  0.01194770708680153\n",
      "116 train_loss:  0.01173341739922762\n",
      "117 train_loss:  0.01154613085091114\n",
      "118 train_loss:  0.01137967400252819\n",
      "119 train_loss:  0.011188418343663216\n",
      "119 test_loss:  0.1514175233617425\n",
      "120 train_loss:  0.010976870097219944\n",
      "121 train_loss:  0.010826067049056292\n",
      "122 train_loss:  0.010640015676617622\n",
      "123 train_loss:  0.010415152311325074\n",
      "124 train_loss:  0.010318039432168007\n",
      "125 train_loss:  0.010143634788691997\n",
      "126 train_loss:  0.009922152645885944\n",
      "127 train_loss:  0.009784371238201856\n",
      "128 train_loss:  0.00963105607777834\n",
      "129 train_loss:  0.009483232349157333\n",
      "130 train_loss:  0.009290666729211807\n",
      "131 train_loss:  0.009168444089591504\n",
      "132 train_loss:  0.009032668061554432\n",
      "133 train_loss:  0.008852113671600818\n",
      "134 train_loss:  0.008691218588501215\n",
      "135 train_loss:  0.008603390641510486\n",
      "136 train_loss:  0.008430902063846588\n",
      "137 train_loss:  0.008318731263279915\n",
      "138 train_loss:  0.008141321539878845\n",
      "139 train_loss:  0.008008636310696602\n",
      "139 test_loss:  0.10715260481461883\n",
      "140 train_loss:  0.007897030785679817\n",
      "141 train_loss:  0.007777125611901283\n",
      "142 train_loss:  0.007635406367480755\n",
      "143 train_loss:  0.007519002389162779\n",
      "144 train_loss:  0.007371854204684496\n",
      "145 train_loss:  0.00725623270496726\n",
      "146 train_loss:  0.007159846294671297\n",
      "147 train_loss:  0.007045101430267095\n",
      "148 train_loss:  0.0069475406594574456\n",
      "149 train_loss:  0.006835292633622885\n",
      "150 train_loss:  0.006733658630400896\n",
      "151 train_loss:  0.006611366961151361\n",
      "152 train_loss:  0.006516270115971565\n",
      "153 train_loss:  0.006402410808950662\n",
      "154 train_loss:  0.006339351013302803\n",
      "155 train_loss:  0.0062094696611166\n",
      "156 train_loss:  0.006099940296262502\n",
      "157 train_loss:  0.00601568890735507\n",
      "158 train_loss:  0.005920090936124325\n",
      "159 train_loss:  0.005813588984310627\n",
      "159 test_loss:  0.07618181645870209\n",
      "160 train_loss:  0.0057310015521943565\n",
      "161 train_loss:  0.0056690570525825025\n",
      "162 train_loss:  0.005568460896611214\n",
      "163 train_loss:  0.005468014795333147\n",
      "164 train_loss:  0.0053923001885414126\n",
      "165 train_loss:  0.005322297345846891\n",
      "166 train_loss:  0.005241821287199855\n",
      "167 train_loss:  0.005152236688882112\n",
      "168 train_loss:  0.005094403102993965\n",
      "169 train_loss:  0.005003037378191948\n",
      "170 train_loss:  0.004926717784255743\n",
      "171 train_loss:  0.004839221760630607\n",
      "172 train_loss:  0.004793488867580891\n",
      "173 train_loss:  0.004696082789450884\n",
      "174 train_loss:  0.004638480842113495\n",
      "175 train_loss:  0.004581140112131834\n",
      "176 train_loss:  0.004535179454833269\n",
      "177 train_loss:  0.0044291306845843795\n",
      "178 train_loss:  0.004381520552560687\n",
      "179 train_loss:  0.004309881404042244\n",
      "179 test_loss:  0.05390571679919958\n",
      "180 train_loss:  0.00424681980162859\n",
      "181 train_loss:  0.004187386333942413\n",
      "182 train_loss:  0.004112482834607363\n",
      "183 train_loss:  0.004088408313691616\n",
      "184 train_loss:  0.004010654473677278\n",
      "185 train_loss:  0.003951692534610629\n",
      "186 train_loss:  0.0038975353166460992\n",
      "187 train_loss:  0.0038641908951103686\n",
      "188 train_loss:  0.003790707644075155\n",
      "189 train_loss:  0.003763457108289003\n",
      "190 train_loss:  0.003712134789675474\n",
      "191 train_loss:  0.0036387476697564126\n",
      "192 train_loss:  0.0035940299229696394\n",
      "193 train_loss:  0.0035368144046515225\n",
      "194 train_loss:  0.003476643357425928\n",
      "195 train_loss:  0.003453620430082083\n",
      "196 train_loss:  0.003389345947653055\n",
      "197 train_loss:  0.0033491516672074797\n",
      "198 train_loss:  0.0032937139831483364\n",
      "199 train_loss:  0.0032561425957828762\n",
      "199 test_loss:  0.03830362455919385\n",
      "200 train_loss:  0.0032139655575156214\n",
      "201 train_loss:  0.0031632246635854245\n",
      "202 train_loss:  0.0031490184552967547\n",
      "203 train_loss:  0.0031049483548849822\n",
      "204 train_loss:  0.0030675047542899846\n",
      "205 train_loss:  0.0030097894836217166\n",
      "206 train_loss:  0.0029793358221650123\n",
      "207 train_loss:  0.0029357613809406755\n",
      "208 train_loss:  0.0028931294567883016\n",
      "209 train_loss:  0.002855183416977525\n",
      "210 train_loss:  0.002827921863645315\n",
      "211 train_loss:  0.0027841923595406117\n",
      "212 train_loss:  0.0027414416638202963\n",
      "213 train_loss:  0.002724333396181464\n",
      "214 train_loss:  0.0026748968847095968\n",
      "215 train_loss:  0.002655697981826961\n",
      "216 train_loss:  0.002634265525266528\n",
      "217 train_loss:  0.002594824859406799\n",
      "218 train_loss:  0.002549746884033084\n",
      "219 train_loss:  0.002529108370654285\n",
      "219 test_loss:  0.02738706289790571\n",
      "220 train_loss:  0.0024977232795208694\n",
      "221 train_loss:  0.0024693089071661234\n",
      "222 train_loss:  0.0024427422415465114\n",
      "223 train_loss:  0.002420775769278407\n",
      "224 train_loss:  0.002379353009164333\n",
      "225 train_loss:  0.0023632052727043627\n",
      "226 train_loss:  0.002320553055033088\n",
      "227 train_loss:  0.0023071470204740763\n",
      "228 train_loss:  0.0022729323897510765\n",
      "229 train_loss:  0.002258029286749661\n",
      "230 train_loss:  0.002221907526254654\n",
      "231 train_loss:  0.0021919635962694885\n",
      "232 train_loss:  0.0021761091332882644\n",
      "233 train_loss:  0.002149656778201461\n",
      "234 train_loss:  0.0021253673871979117\n",
      "235 train_loss:  0.002102559986524284\n",
      "236 train_loss:  0.0020848640450276436\n",
      "237 train_loss:  0.002063751006498933\n",
      "238 train_loss:  0.0020453484635800123\n",
      "239 train_loss:  0.002019327594898641\n",
      "239 test_loss:  0.01975522701628506\n",
      "240 train_loss:  0.0020072658034041525\n",
      "241 train_loss:  0.001973776500672102\n",
      "242 train_loss:  0.001956126317381859\n",
      "243 train_loss:  0.0019406058080494405\n",
      "244 train_loss:  0.0019178384961560369\n",
      "245 train_loss:  0.001911858655512333\n",
      "246 train_loss:  0.0018810603208839892\n",
      "247 train_loss:  0.0018631612882018089\n",
      "248 train_loss:  0.001858623493462801\n",
      "249 train_loss:  0.0018255230132490396\n",
      "250 train_loss:  0.0018142212461680174\n",
      "251 train_loss:  0.0017985713854432107\n",
      "252 train_loss:  0.001783970957621932\n",
      "253 train_loss:  0.0017579352529719472\n",
      "254 train_loss:  0.0017543703131377697\n",
      "255 train_loss:  0.0017242654133588075\n",
      "256 train_loss:  0.0017171041294932366\n",
      "257 train_loss:  0.0017027206998318433\n",
      "258 train_loss:  0.0016889628255739807\n",
      "259 train_loss:  0.001670599509961903\n",
      "259 test_loss:  0.014319005687721074\n",
      "260 train_loss:  0.0016533316764980556\n",
      "261 train_loss:  0.0016409705951809883\n",
      "262 train_loss:  0.00162806810811162\n",
      "263 train_loss:  0.001615816978737712\n",
      "264 train_loss:  0.001596033265814185\n",
      "265 train_loss:  0.0015829178504645825\n",
      "266 train_loss:  0.0015770419966429472\n",
      "267 train_loss:  0.001558079607784748\n",
      "268 train_loss:  0.001553093739785254\n",
      "269 train_loss:  0.0015374369733035563\n",
      "270 train_loss:  0.0015239862818270922\n",
      "271 train_loss:  0.0015118792792782187\n",
      "272 train_loss:  0.0015016249672044069\n",
      "273 train_loss:  0.0014873053273186088\n",
      "274 train_loss:  0.0014759934274479747\n",
      "275 train_loss:  0.0014639224763959647\n",
      "276 train_loss:  0.001451485794968903\n",
      "277 train_loss:  0.001449827360920608\n",
      "278 train_loss:  0.0014339414611458778\n",
      "279 train_loss:  0.0014243622147478163\n",
      "279 test_loss:  0.010517579838633537\n",
      "280 train_loss:  0.0014129070797935129\n",
      "281 train_loss:  0.0014034789055585861\n",
      "282 train_loss:  0.0013987126108258962\n",
      "283 train_loss:  0.001383538912050426\n",
      "284 train_loss:  0.0013764312118291855\n",
      "285 train_loss:  0.0013677064841613174\n",
      "286 train_loss:  0.0013551377644762397\n",
      "287 train_loss:  0.001352000329643488\n",
      "288 train_loss:  0.0013416438922286033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 train_loss:  0.0013332034554332495\n",
      "290 train_loss:  0.0013227536575868726\n",
      "291 train_loss:  0.0013170837517827748\n",
      "292 train_loss:  0.001308416910469532\n",
      "293 train_loss:  0.0012959805689752102\n",
      "294 train_loss:  0.0012922201305627823\n",
      "295 train_loss:  0.0012820965657010675\n",
      "296 train_loss:  0.0012792141688987612\n",
      "297 train_loss:  0.0012702751951292157\n",
      "298 train_loss:  0.0012618678295984863\n",
      "299 train_loss:  0.0012547606509178877\n",
      "299 test_loss:  0.007889048019424081\n",
      "300 train_loss:  0.0012538850540295243\n",
      "301 train_loss:  0.0012410933105275034\n",
      "302 train_loss:  0.0012330346251837908\n",
      "303 train_loss:  0.0012295601982623338\n",
      "304 train_loss:  0.0012208118941634894\n",
      "305 train_loss:  0.0012145258206874131\n",
      "306 train_loss:  0.0012099903612397612\n",
      "307 train_loss:  0.001203360054641962\n",
      "308 train_loss:  0.0011971033504232765\n",
      "309 train_loss:  0.001192372664809227\n",
      "310 train_loss:  0.0011874817032366991\n",
      "311 train_loss:  0.001178813287988305\n",
      "312 train_loss:  0.001174336215481162\n",
      "313 train_loss:  0.001165952207520604\n",
      "314 train_loss:  0.0011619559954851866\n",
      "315 train_loss:  0.001158607737161219\n",
      "316 train_loss:  0.0011517768073827028\n",
      "317 train_loss:  0.001144862282089889\n",
      "318 train_loss:  0.0011424829671159388\n",
      "319 train_loss:  0.0011363797960802912\n",
      "319 test_loss:  0.005984492301940918\n",
      "320 train_loss:  0.0011306273750960826\n",
      "321 train_loss:  0.0011278255935758352\n",
      "322 train_loss:  0.001123678230214864\n",
      "323 train_loss:  0.0011210527783259748\n",
      "324 train_loss:  0.001113166706636548\n",
      "325 train_loss:  0.001110016549937427\n",
      "326 train_loss:  0.0011059767473489046\n",
      "327 train_loss:  0.0010983907617628575\n",
      "328 train_loss:  0.0010963349416851997\n",
      "329 train_loss:  0.0010908355889841913\n",
      "330 train_loss:  0.0010883774189278483\n",
      "331 train_loss:  0.0010863089095801116\n",
      "332 train_loss:  0.001079901228658855\n",
      "333 train_loss:  0.001078425347805023\n",
      "334 train_loss:  0.0010712012485601008\n",
      "335 train_loss:  0.0010694296192377806\n",
      "336 train_loss:  0.001066322848200798\n",
      "337 train_loss:  0.0010629823058843613\n",
      "338 train_loss:  0.001058942899107933\n",
      "339 train_loss:  0.001053595240227878\n",
      "339 test_loss:  0.004604609538801014\n",
      "340 train_loss:  0.0010508387722074986\n",
      "341 train_loss:  0.0010466651432216166\n",
      "342 train_loss:  0.0010467284452170135\n",
      "343 train_loss:  0.0010425102524459363\n",
      "344 train_loss:  0.0010411034710705281\n",
      "345 train_loss:  0.0010372159304097295\n",
      "346 train_loss:  0.0010350604820996523\n",
      "347 train_loss:  0.0010298799024894834\n",
      "348 train_loss:  0.0010277304332703351\n",
      "349 train_loss:  0.0010248797480016947\n",
      "350 train_loss:  0.0010224910406395793\n",
      "351 train_loss:  0.0010171816684305668\n",
      "352 train_loss:  0.001014759372919798\n",
      "353 train_loss:  0.0010148045374080538\n",
      "354 train_loss:  0.0010116727557033301\n",
      "355 train_loss:  0.0010094190761446953\n",
      "356 train_loss:  0.0010071275196969509\n",
      "357 train_loss:  0.001003217026591301\n",
      "358 train_loss:  0.0010027300892397762\n",
      "359 train_loss:  0.0010003215749748052\n",
      "359 test_loss:  0.0036619537556543944\n",
      "360 train_loss:  0.0009965078020468354\n",
      "361 train_loss:  0.0009939398732967675\n",
      "362 train_loss:  0.0009923790628090501\n",
      "363 train_loss:  0.000991807235404849\n",
      "364 train_loss:  0.0009893369488418102\n",
      "365 train_loss:  0.0009869310935027896\n",
      "366 train_loss:  0.000982502093538642\n",
      "367 train_loss:  0.0009828904503956437\n",
      "368 train_loss:  0.0009784510009922087\n",
      "369 train_loss:  0.0009768313262611627\n",
      "370 train_loss:  0.0009739622240886092\n",
      "371 train_loss:  0.0009726957557722926\n",
      "372 train_loss:  0.0009709766414016485\n",
      "373 train_loss:  0.0009684238024055958\n",
      "374 train_loss:  0.000971205374225974\n",
      "375 train_loss:  0.0009663586085662246\n",
      "376 train_loss:  0.0009644334972836077\n",
      "377 train_loss:  0.0009607771574519575\n",
      "378 train_loss:  0.0009606090281158685\n",
      "379 train_loss:  0.0009602960245683789\n",
      "379 test_loss:  0.0029872741131111978\n",
      "380 train_loss:  0.0009586776839569211\n",
      "381 train_loss:  0.0009597351960837841\n",
      "382 train_loss:  0.000954789062961936\n",
      "383 train_loss:  0.0009560932870954275\n",
      "384 train_loss:  0.0009515390638262034\n",
      "385 train_loss:  0.000950333452783525\n",
      "386 train_loss:  0.0009494298556819559\n",
      "387 train_loss:  0.0009474640712141991\n",
      "388 train_loss:  0.0009453394263982773\n",
      "389 train_loss:  0.0009453280176967382\n",
      "390 train_loss:  0.000940142972394824\n",
      "391 train_loss:  0.0009414490731433034\n",
      "392 train_loss:  0.0009415406780317426\n",
      "393 train_loss:  0.0009404124575667083\n",
      "394 train_loss:  0.00093890271615237\n",
      "395 train_loss:  0.0009355333959683776\n",
      "396 train_loss:  0.0009354574047029018\n",
      "397 train_loss:  0.0009333275025710464\n",
      "398 train_loss:  0.0009335649199783802\n",
      "399 train_loss:  0.0009362454758957029\n",
      "399 test_loss:  0.0024816273152828217\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    train_loss, _ = train(net, train_loader, loss, optimizer)\n",
    "    print(epoch, \"train_loss: \", train_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        test_loss, _ = evaluate_test(net, test_loader, loss)\n",
    "        print(epoch, \"test_loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7709e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0158,  1.2257, -3.3884,  5.4836]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41d52e",
   "metadata": {},
   "source": [
    "### 欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bcabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train_loss:  2.9889348411560057\n",
      "1 train_loss:  2.9889348793029784\n",
      "2 train_loss:  2.98893479347229\n",
      "3 train_loss:  2.9889348220825194\n",
      "4 train_loss:  2.9889347457885744\n",
      "5 train_loss:  2.9889348793029784\n",
      "6 train_loss:  2.988934860229492\n",
      "7 train_loss:  2.9889348220825194\n",
      "8 train_loss:  2.9889348220825194\n",
      "9 train_loss:  2.9889348411560057\n",
      "10 train_loss:  2.988934803009033\n",
      "11 train_loss:  2.988934917449951\n",
      "12 train_loss:  2.9889347648620603\n",
      "13 train_loss:  2.988934860229492\n",
      "14 train_loss:  2.9889348793029784\n",
      "15 train_loss:  2.9889347648620603\n",
      "16 train_loss:  2.988934917449951\n",
      "17 train_loss:  2.9889348411560057\n",
      "18 train_loss:  2.9889347457885744\n",
      "19 train_loss:  2.9889347648620603\n",
      "19 test_loss:  4.985845699310302\n",
      "20 train_loss:  2.9889347457885744\n",
      "21 train_loss:  2.9889347839355467\n",
      "22 train_loss:  2.9889348983764648\n",
      "23 train_loss:  2.9889348220825194\n",
      "24 train_loss:  2.9889348411560057\n",
      "25 train_loss:  2.988934803009033\n",
      "26 train_loss:  2.9889347839355467\n",
      "27 train_loss:  2.9889347839355467\n",
      "28 train_loss:  2.9889348411560057\n",
      "29 train_loss:  2.9889349365234374\n",
      "30 train_loss:  2.9889347839355467\n",
      "31 train_loss:  2.988934803009033\n",
      "32 train_loss:  2.9889348220825194\n",
      "33 train_loss:  2.9889347457885744\n",
      "34 train_loss:  2.9889348411560057\n",
      "35 train_loss:  2.9889348220825194\n",
      "36 train_loss:  2.9889349365234374\n",
      "37 train_loss:  2.9889348411560057\n",
      "38 train_loss:  2.9889348220825194\n",
      "39 train_loss:  2.9889348125457764\n",
      "39 test_loss:  4.985845623016357\n",
      "40 train_loss:  2.9889347457885744\n",
      "41 train_loss:  2.988934860229492\n",
      "42 train_loss:  2.988934860229492\n",
      "43 train_loss:  2.9889348411560057\n",
      "44 train_loss:  2.9889347839355467\n",
      "45 train_loss:  2.9889348411560057\n",
      "46 train_loss:  2.988934860229492\n",
      "47 train_loss:  2.988934803009033\n",
      "48 train_loss:  2.988934803009033\n",
      "49 train_loss:  2.988934917449951\n",
      "50 train_loss:  2.9889347839355467\n",
      "51 train_loss:  2.9889347648620603\n",
      "52 train_loss:  2.988934803009033\n",
      "53 train_loss:  2.9889349365234374\n",
      "54 train_loss:  2.988934803009033\n",
      "55 train_loss:  2.9889348220825194\n",
      "56 train_loss:  2.9889348220825194\n",
      "57 train_loss:  2.988934860229492\n",
      "58 train_loss:  2.988934860229492\n",
      "59 train_loss:  2.9889348983764648\n",
      "59 test_loss:  4.985845623016357\n",
      "60 train_loss:  2.9889348411560057\n",
      "61 train_loss:  2.9889348220825194\n",
      "62 train_loss:  2.988934860229492\n",
      "63 train_loss:  2.9889347457885744\n",
      "64 train_loss:  2.9889348220825194\n",
      "65 train_loss:  2.988934917449951\n",
      "66 train_loss:  2.9889347839355467\n",
      "67 train_loss:  2.988934803009033\n",
      "68 train_loss:  2.988934860229492\n",
      "69 train_loss:  2.988934726715088\n",
      "70 train_loss:  2.9889348411560057\n",
      "71 train_loss:  2.9889347839355467\n",
      "72 train_loss:  2.988934803009033\n",
      "73 train_loss:  2.9889348220825194\n",
      "74 train_loss:  2.988934860229492\n",
      "75 train_loss:  2.988934803009033\n",
      "76 train_loss:  2.988934803009033\n",
      "77 train_loss:  2.9889348316192628\n",
      "78 train_loss:  2.9889347839355467\n",
      "79 train_loss:  2.988934917449951\n",
      "79 test_loss:  4.985845699310302\n",
      "80 train_loss:  2.98893497467041\n",
      "81 train_loss:  2.988934955596924\n",
      "82 train_loss:  2.9889348220825194\n",
      "83 train_loss:  2.988934860229492\n",
      "84 train_loss:  2.9889348220825194\n",
      "85 train_loss:  2.9889348411560057\n",
      "86 train_loss:  2.9889347839355467\n",
      "87 train_loss:  2.9889348793029784\n",
      "88 train_loss:  2.988934888839722\n",
      "89 train_loss:  2.9889348411560057\n",
      "90 train_loss:  2.9889348220825194\n",
      "91 train_loss:  2.988934803009033\n",
      "92 train_loss:  2.9889348793029784\n",
      "93 train_loss:  2.988934860229492\n",
      "94 train_loss:  2.988934860229492\n",
      "95 train_loss:  2.9889348411560057\n",
      "96 train_loss:  2.9889348793029784\n",
      "97 train_loss:  2.988934803009033\n",
      "98 train_loss:  2.988934803009033\n",
      "99 train_loss:  2.988934860229492\n",
      "99 test_loss:  4.985845603942871\n",
      "100 train_loss:  2.9889348983764648\n",
      "101 train_loss:  2.988934860229492\n",
      "102 train_loss:  2.988934803009033\n",
      "103 train_loss:  2.988934860229492\n",
      "104 train_loss:  2.988934803009033\n",
      "105 train_loss:  2.9889348411560057\n",
      "106 train_loss:  2.9889347839355467\n",
      "107 train_loss:  2.9889348411560057\n",
      "108 train_loss:  2.9889348793029784\n",
      "109 train_loss:  2.988934726715088\n",
      "110 train_loss:  2.988934803009033\n",
      "111 train_loss:  2.9889348983764648\n",
      "112 train_loss:  2.9889347839355467\n",
      "113 train_loss:  2.988934803009033\n",
      "114 train_loss:  2.9889348793029784\n",
      "115 train_loss:  2.9889348220825194\n",
      "116 train_loss:  2.988934907913208\n",
      "117 train_loss:  2.9889348125457764\n",
      "118 train_loss:  2.9889348793029784\n",
      "119 train_loss:  2.988934860229492\n",
      "119 test_loss:  4.9858455467224125\n",
      "120 train_loss:  2.988934917449951\n",
      "121 train_loss:  2.9889347076416017\n",
      "122 train_loss:  2.9889347457885744\n",
      "123 train_loss:  2.9889348220825194\n",
      "124 train_loss:  2.9889348983764648\n",
      "125 train_loss:  2.988934803009033\n",
      "126 train_loss:  2.9889348983764648\n",
      "127 train_loss:  2.9889348411560057\n",
      "128 train_loss:  2.988934917449951\n",
      "129 train_loss:  2.9889348793029784\n",
      "130 train_loss:  2.9889348220825194\n",
      "131 train_loss:  2.988934803009033\n",
      "132 train_loss:  2.9889347839355467\n",
      "133 train_loss:  2.9889348220825194\n",
      "134 train_loss:  2.988934860229492\n",
      "135 train_loss:  2.9889348983764648\n",
      "136 train_loss:  2.9889347743988037\n",
      "137 train_loss:  2.9889348411560057\n",
      "138 train_loss:  2.9889348411560057\n",
      "139 train_loss:  2.988934803009033\n",
      "139 test_loss:  4.985845603942871\n",
      "140 train_loss:  2.9889348983764648\n",
      "141 train_loss:  2.9889348220825194\n",
      "142 train_loss:  2.9889348793029784\n",
      "143 train_loss:  2.9889348411560057\n",
      "144 train_loss:  2.9889348793029784\n",
      "145 train_loss:  2.988934726715088\n",
      "146 train_loss:  2.988934803009033\n",
      "147 train_loss:  2.9889348793029784\n",
      "148 train_loss:  2.9889348983764648\n",
      "149 train_loss:  2.9889348125457764\n",
      "150 train_loss:  2.9889349365234374\n",
      "151 train_loss:  2.9889348983764648\n",
      "152 train_loss:  2.9889347839355467\n",
      "153 train_loss:  2.9889347457885744\n",
      "154 train_loss:  2.988934917449951\n",
      "155 train_loss:  2.9889348793029784\n",
      "156 train_loss:  2.988934860229492\n",
      "157 train_loss:  2.9889348411560057\n",
      "158 train_loss:  2.988934860229492\n",
      "159 train_loss:  2.9889348983764648\n",
      "159 test_loss:  4.985845355987549\n",
      "160 train_loss:  2.988934669494629\n",
      "161 train_loss:  2.988934803009033\n",
      "162 train_loss:  2.988934803009033\n",
      "163 train_loss:  2.9889347648620603\n",
      "164 train_loss:  2.9889347457885744\n",
      "165 train_loss:  2.9889348411560057\n",
      "166 train_loss:  2.988934860229492\n",
      "167 train_loss:  2.9889348793029784\n",
      "168 train_loss:  2.9889348220825194\n",
      "169 train_loss:  2.9889348220825194\n",
      "170 train_loss:  2.9889348983764648\n",
      "171 train_loss:  2.9889348983764648\n",
      "172 train_loss:  2.988934888839722\n",
      "173 train_loss:  2.9889347839355467\n",
      "174 train_loss:  2.9889348411560057\n",
      "175 train_loss:  2.988934888839722\n",
      "176 train_loss:  2.9889348983764648\n",
      "177 train_loss:  2.9889348411560057\n",
      "178 train_loss:  2.9889348411560057\n",
      "179 train_loss:  2.988934860229492\n",
      "179 test_loss:  4.985845794677735\n",
      "180 train_loss:  2.988934955596924\n",
      "181 train_loss:  2.988934860229492\n",
      "182 train_loss:  2.988934803009033\n",
      "183 train_loss:  2.9889348220825194\n",
      "184 train_loss:  2.988934917449951\n",
      "185 train_loss:  2.988934860229492\n",
      "186 train_loss:  2.9889347839355467\n",
      "187 train_loss:  2.9889348220825194\n",
      "188 train_loss:  2.9889347457885744\n",
      "189 train_loss:  2.9889348411560057\n",
      "190 train_loss:  2.9889347839355467\n",
      "191 train_loss:  2.988934803009033\n",
      "192 train_loss:  2.9889347839355467\n",
      "193 train_loss:  2.9889347076416017\n",
      "194 train_loss:  2.988934860229492\n",
      "195 train_loss:  2.9889347839355467\n",
      "196 train_loss:  2.988934860229492\n",
      "197 train_loss:  2.988934803009033\n",
      "198 train_loss:  2.9889348411560057\n",
      "199 train_loss:  2.9889347839355467\n",
      "199 test_loss:  4.985845565795898\n",
      "200 train_loss:  2.9889348411560057\n",
      "201 train_loss:  2.9889348793029784\n",
      "202 train_loss:  2.9889347648620603\n",
      "203 train_loss:  2.9889348220825194\n",
      "204 train_loss:  2.988934803009033\n",
      "205 train_loss:  2.9889347648620603\n",
      "206 train_loss:  2.9889347076416017\n",
      "207 train_loss:  2.9889348793029784\n",
      "208 train_loss:  2.9889348220825194\n",
      "209 train_loss:  2.9889347553253174\n",
      "210 train_loss:  2.988934803009033\n",
      "211 train_loss:  2.9889348793029784\n",
      "212 train_loss:  2.9889347839355467\n",
      "213 train_loss:  2.9889348983764648\n",
      "214 train_loss:  2.9889348220825194\n",
      "215 train_loss:  2.9889348793029784\n",
      "216 train_loss:  2.9889348793029784\n",
      "217 train_loss:  2.9889347648620603\n",
      "218 train_loss:  2.988934803009033\n",
      "219 train_loss:  2.9889348983764648\n",
      "219 test_loss:  4.985845584869384\n",
      "220 train_loss:  2.9889348411560057\n",
      "221 train_loss:  2.9889347457885744\n",
      "222 train_loss:  2.9889348220825194\n",
      "223 train_loss:  2.9889347076416017\n",
      "224 train_loss:  2.988934726715088\n",
      "225 train_loss:  2.988934803009033\n",
      "226 train_loss:  2.9889348125457764\n",
      "227 train_loss:  2.9889348793029784\n",
      "228 train_loss:  2.988934803009033\n",
      "229 train_loss:  2.988934917449951\n",
      "230 train_loss:  2.988934955596924\n",
      "231 train_loss:  2.988934860229492\n",
      "232 train_loss:  2.9889348411560057\n",
      "233 train_loss:  2.9889348220825194\n",
      "234 train_loss:  2.9889348220825194\n",
      "235 train_loss:  2.9889347648620603\n",
      "236 train_loss:  2.9889347553253174\n",
      "237 train_loss:  2.9889348411560057\n",
      "238 train_loss:  2.988934860229492\n",
      "239 train_loss:  2.9889347839355467\n",
      "239 test_loss:  4.9858456802368165\n",
      "240 train_loss:  2.988934860229492\n",
      "241 train_loss:  2.988934860229492\n",
      "242 train_loss:  2.988934860229492\n",
      "243 train_loss:  2.9889347839355467\n",
      "244 train_loss:  2.9889348793029784\n",
      "245 train_loss:  2.988934860229492\n",
      "246 train_loss:  2.9889347457885744\n",
      "247 train_loss:  2.988934803009033\n",
      "248 train_loss:  2.988934803009033\n",
      "249 train_loss:  2.9889348793029784\n",
      "250 train_loss:  2.9889348983764648\n",
      "251 train_loss:  2.9889347457885744\n",
      "252 train_loss:  2.9889348411560057\n",
      "253 train_loss:  2.9889348793029784\n",
      "254 train_loss:  2.9889348793029784\n",
      "255 train_loss:  2.988934726715088\n",
      "256 train_loss:  2.9889348220825194\n",
      "257 train_loss:  2.9889348125457764\n",
      "258 train_loss:  2.988934803009033\n",
      "259 train_loss:  2.9889348983764648\n",
      "259 test_loss:  4.985845756530762\n",
      "260 train_loss:  2.9889348411560057\n",
      "261 train_loss:  2.988934860229492\n",
      "262 train_loss:  2.988934860229492\n",
      "263 train_loss:  2.9889348793029784\n",
      "264 train_loss:  2.988934860229492\n",
      "265 train_loss:  2.9889348411560057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 train_loss:  2.9889347839355467\n",
      "267 train_loss:  2.9889348411560057\n",
      "268 train_loss:  2.9889347457885744\n",
      "269 train_loss:  2.9889347457885744\n",
      "270 train_loss:  2.9889347839355467\n",
      "271 train_loss:  2.9889347839355467\n",
      "272 train_loss:  2.9889347839355467\n",
      "273 train_loss:  2.9889347457885744\n",
      "274 train_loss:  2.9889347648620603\n",
      "275 train_loss:  2.988934803009033\n",
      "276 train_loss:  2.9889347839355467\n",
      "277 train_loss:  2.9889347839355467\n",
      "278 train_loss:  2.988934917449951\n",
      "279 train_loss:  2.9889347648620603\n",
      "279 test_loss:  4.985845718383789\n",
      "280 train_loss:  2.988934917449951\n",
      "281 train_loss:  2.9889347839355467\n",
      "282 train_loss:  2.9889348793029784\n",
      "283 train_loss:  2.9889348411560057\n",
      "284 train_loss:  2.9889347839355467\n",
      "285 train_loss:  2.9889347839355467\n",
      "286 train_loss:  2.9889348983764648\n",
      "287 train_loss:  2.988934860229492\n",
      "288 train_loss:  2.9889347457885744\n",
      "289 train_loss:  2.988934860229492\n",
      "290 train_loss:  2.988934917449951\n",
      "291 train_loss:  2.9889347839355467\n",
      "292 train_loss:  2.9889347743988037\n",
      "293 train_loss:  2.9889347839355467\n",
      "294 train_loss:  2.988934860229492\n",
      "295 train_loss:  2.9889349365234374\n",
      "296 train_loss:  2.988934803009033\n",
      "297 train_loss:  2.9889348793029784\n",
      "298 train_loss:  2.9889348220825194\n",
      "299 train_loss:  2.9889348793029784\n",
      "299 test_loss:  4.985845851898193\n",
      "300 train_loss:  2.9889347839355467\n",
      "301 train_loss:  2.9889348220825194\n",
      "302 train_loss:  2.9889348793029784\n",
      "303 train_loss:  2.9889348220825194\n",
      "304 train_loss:  2.988934803009033\n",
      "305 train_loss:  2.988934917449951\n",
      "306 train_loss:  2.988934860229492\n",
      "307 train_loss:  2.988934803009033\n",
      "308 train_loss:  2.9889347839355467\n",
      "309 train_loss:  2.9889349269866945\n",
      "310 train_loss:  2.9889347648620603\n",
      "311 train_loss:  2.9889347457885744\n",
      "312 train_loss:  2.9889348220825194\n",
      "313 train_loss:  2.9889347648620603\n",
      "314 train_loss:  2.9889348411560057\n",
      "315 train_loss:  2.9889348411560057\n",
      "316 train_loss:  2.9889348411560057\n",
      "317 train_loss:  2.9889348411560057\n",
      "318 train_loss:  2.9889347648620603\n",
      "319 train_loss:  2.9889348793029784\n",
      "319 test_loss:  4.985845642089844\n",
      "320 train_loss:  2.9889348411560057\n",
      "321 train_loss:  2.9889348793029784\n",
      "322 train_loss:  2.9889347839355467\n",
      "323 train_loss:  2.9889348125457764\n",
      "324 train_loss:  2.9889347457885744\n",
      "325 train_loss:  2.988934803009033\n",
      "326 train_loss:  2.9889348220825194\n",
      "327 train_loss:  2.9889348793029784\n",
      "328 train_loss:  2.9889348220825194\n",
      "329 train_loss:  2.9889348793029784\n",
      "330 train_loss:  2.9889348983764648\n",
      "331 train_loss:  2.9889348411560057\n",
      "332 train_loss:  2.9889348793029784\n",
      "333 train_loss:  2.988934803009033\n",
      "334 train_loss:  2.9889348793029784\n",
      "335 train_loss:  2.988934917449951\n",
      "336 train_loss:  2.9889348983764648\n",
      "337 train_loss:  2.9889348220825194\n",
      "338 train_loss:  2.9889347839355467\n",
      "339 train_loss:  2.9889349365234374\n",
      "339 test_loss:  4.9858458137512205\n",
      "340 train_loss:  2.988934860229492\n",
      "341 train_loss:  2.988934917449951\n",
      "342 train_loss:  2.9889348983764648\n",
      "343 train_loss:  2.9889348411560057\n",
      "344 train_loss:  2.9889347839355467\n",
      "345 train_loss:  2.9889347648620603\n",
      "346 train_loss:  2.9889347839355467\n",
      "347 train_loss:  2.9889348411560057\n",
      "348 train_loss:  2.98893479347229\n",
      "349 train_loss:  2.9889347839355467\n",
      "350 train_loss:  2.988934726715088\n",
      "351 train_loss:  2.9889347457885744\n",
      "352 train_loss:  2.988934726715088\n",
      "353 train_loss:  2.988934803009033\n",
      "354 train_loss:  2.9889347457885744\n",
      "355 train_loss:  2.9889348793029784\n",
      "356 train_loss:  2.988934860229492\n",
      "357 train_loss:  2.9889348411560057\n",
      "358 train_loss:  2.988934803009033\n",
      "359 train_loss:  2.9889347839355467\n",
      "359 test_loss:  4.985845527648926\n",
      "360 train_loss:  2.9889348411560057\n",
      "361 train_loss:  2.988934860229492\n",
      "362 train_loss:  2.988934803009033\n",
      "363 train_loss:  2.9889348411560057\n",
      "364 train_loss:  2.988934803009033\n",
      "365 train_loss:  2.9889348411560057\n",
      "366 train_loss:  2.9889348220825194\n",
      "367 train_loss:  2.9889349365234374\n",
      "368 train_loss:  2.9889348411560057\n",
      "369 train_loss:  2.988934803009033\n",
      "370 train_loss:  2.9889348793029784\n",
      "371 train_loss:  2.988934955596924\n",
      "372 train_loss:  2.9889348793029784\n",
      "373 train_loss:  2.9889347457885744\n",
      "374 train_loss:  2.988934860229492\n",
      "375 train_loss:  2.988934860229492\n",
      "376 train_loss:  2.9889348983764648\n",
      "377 train_loss:  2.9889348983764648\n",
      "378 train_loss:  2.988934860229492\n",
      "379 train_loss:  2.9889348220825194\n",
      "379 test_loss:  4.98584587097168\n",
      "380 train_loss:  2.9889348411560057\n",
      "381 train_loss:  2.9889348220825194\n",
      "382 train_loss:  2.988934917449951\n",
      "383 train_loss:  2.988934860229492\n",
      "384 train_loss:  2.9889347457885744\n",
      "385 train_loss:  2.988934860229492\n",
      "386 train_loss:  2.9889347839355467\n",
      "387 train_loss:  2.9889347457885744\n",
      "388 train_loss:  2.988934860229492\n",
      "389 train_loss:  2.988934917449951\n",
      "390 train_loss:  2.988934917449951\n",
      "391 train_loss:  2.988934917449951\n",
      "392 train_loss:  2.9889348411560057\n",
      "393 train_loss:  2.988934955596924\n",
      "394 train_loss:  2.9889348220825194\n",
      "395 train_loss:  2.988934860229492\n",
      "396 train_loss:  2.9889348220825194\n",
      "397 train_loss:  2.9889347457885744\n",
      "398 train_loss:  2.988934803009033\n",
      "399 train_loss:  2.9889347171783447\n",
      "399 test_loss:  4.985845718383789\n"
     ]
    }
   ],
   "source": [
    "input_num = 2\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_num, output_num, bias=False)\n",
    ")\n",
    "train_loader = loadArray(poly_features[:n_train, :input_num], labels[:n_train], batch_size)\n",
    "test_loader = loadArray(poly_features[n_train:, :input_num], labels[n_train:], batch_size)\n",
    "\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    train_loss, _ = train(net, train_loader, loss, optimizer)\n",
    "    print(epoch, \"train_loss: \", train_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        test_loss, _ = evaluate_test(net, test_loader, loss)\n",
    "        print(epoch, \"test_loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8e8be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2731, -0.3153]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8840bba",
   "metadata": {},
   "source": [
    "### 过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce4ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train_loss:  2.7043732452392577\n",
      "1 train_loss:  2.7043731880187987\n",
      "2 train_loss:  2.704373207092285\n",
      "3 train_loss:  2.7043733406066894\n",
      "4 train_loss:  2.7043731689453123\n",
      "5 train_loss:  2.7043731689453123\n",
      "6 train_loss:  2.704373254776001\n",
      "7 train_loss:  2.7043732261657714\n",
      "8 train_loss:  2.7043731784820557\n",
      "9 train_loss:  2.7043732261657714\n",
      "10 train_loss:  2.7043731689453123\n",
      "11 train_loss:  2.7043731689453123\n",
      "12 train_loss:  2.7043731880187987\n",
      "13 train_loss:  2.7043732833862304\n",
      "14 train_loss:  2.7043733024597167\n",
      "15 train_loss:  2.704373264312744\n",
      "16 train_loss:  2.704373016357422\n",
      "17 train_loss:  2.7043730926513674\n",
      "18 train_loss:  2.7043730926513674\n",
      "19 train_loss:  2.7043731212615967\n",
      "19 test_loss:  4.484903602600098\n",
      "20 train_loss:  2.7043731689453123\n",
      "21 train_loss:  2.7043732261657714\n",
      "22 train_loss:  2.7043731784820557\n",
      "23 train_loss:  2.7043731880187987\n",
      "24 train_loss:  2.704373197555542\n",
      "25 train_loss:  2.7043732833862304\n",
      "26 train_loss:  2.7043731880187987\n",
      "27 train_loss:  2.7043731689453123\n",
      "28 train_loss:  2.7043731689453123\n",
      "29 train_loss:  2.7043731880187987\n",
      "30 train_loss:  2.7043732452392577\n",
      "31 train_loss:  2.7043731117248537\n",
      "32 train_loss:  2.7043732452392577\n",
      "33 train_loss:  2.704373207092285\n",
      "34 train_loss:  2.7043730640411376\n",
      "35 train_loss:  2.7043731307983396\n",
      "36 train_loss:  2.704373264312744\n",
      "37 train_loss:  2.704373207092285\n",
      "38 train_loss:  2.7043731689453123\n",
      "39 train_loss:  2.7043731880187987\n",
      "39 test_loss:  4.484903831481933\n",
      "40 train_loss:  2.7043731117248537\n",
      "41 train_loss:  2.7043732261657714\n",
      "42 train_loss:  2.704373264312744\n",
      "43 train_loss:  2.7043732833862304\n",
      "44 train_loss:  2.704373207092285\n",
      "45 train_loss:  2.7043732261657714\n",
      "46 train_loss:  2.7043731307983396\n",
      "47 train_loss:  2.7043731307983396\n",
      "48 train_loss:  2.704373292922974\n",
      "49 train_loss:  2.7043731307983396\n",
      "50 train_loss:  2.7043732261657714\n",
      "51 train_loss:  2.7043731880187987\n",
      "52 train_loss:  2.7043731594085694\n",
      "53 train_loss:  2.704373149871826\n",
      "54 train_loss:  2.7043732452392577\n",
      "55 train_loss:  2.7043732738494874\n",
      "56 train_loss:  2.7043731880187987\n",
      "57 train_loss:  2.7043731880187987\n",
      "58 train_loss:  2.7043731689453123\n",
      "59 train_loss:  2.7043731594085694\n",
      "59 test_loss:  4.48490348815918\n",
      "60 train_loss:  2.704373149871826\n",
      "61 train_loss:  2.7043732452392577\n",
      "62 train_loss:  2.7043732452392577\n",
      "63 train_loss:  2.7043731880187987\n",
      "64 train_loss:  2.7043731594085694\n",
      "65 train_loss:  2.704373321533203\n",
      "66 train_loss:  2.7043731689453123\n",
      "67 train_loss:  2.7043730926513674\n",
      "68 train_loss:  2.704373073577881\n",
      "69 train_loss:  2.7043731689453123\n",
      "70 train_loss:  2.7043732738494874\n",
      "71 train_loss:  2.704373149871826\n",
      "72 train_loss:  2.7043731594085694\n",
      "73 train_loss:  2.7043731880187987\n",
      "74 train_loss:  2.704373207092285\n",
      "75 train_loss:  2.7043731689453123\n",
      "76 train_loss:  2.7043731307983396\n",
      "77 train_loss:  2.704373207092285\n",
      "78 train_loss:  2.7043731784820557\n",
      "79 train_loss:  2.7043731689453123\n",
      "79 test_loss:  4.484903316497803\n",
      "80 train_loss:  2.704373207092285\n",
      "81 train_loss:  2.704373140335083\n",
      "82 train_loss:  2.704373207092285\n",
      "83 train_loss:  2.7043731689453123\n",
      "84 train_loss:  2.70437331199646\n",
      "85 train_loss:  2.7043731880187987\n",
      "86 train_loss:  2.704373207092285\n",
      "87 train_loss:  2.704373197555542\n",
      "88 train_loss:  2.704373292922974\n",
      "89 train_loss:  2.7043731784820557\n",
      "90 train_loss:  2.7043731880187987\n",
      "91 train_loss:  2.7043731212615967\n",
      "92 train_loss:  2.704373207092285\n",
      "93 train_loss:  2.7043731689453123\n",
      "94 train_loss:  2.7043732261657714\n",
      "95 train_loss:  2.7043731689453123\n",
      "96 train_loss:  2.7043732452392577\n",
      "97 train_loss:  2.704373207092285\n",
      "98 train_loss:  2.704373207092285\n",
      "99 train_loss:  2.7043732738494874\n",
      "99 test_loss:  4.484903469085693\n",
      "100 train_loss:  2.7043732452392577\n",
      "101 train_loss:  2.704373149871826\n",
      "102 train_loss:  2.704373149871826\n",
      "103 train_loss:  2.704373207092285\n",
      "104 train_loss:  2.7043732738494874\n",
      "105 train_loss:  2.7043731880187987\n",
      "106 train_loss:  2.704373264312744\n",
      "107 train_loss:  2.704373149871826\n",
      "108 train_loss:  2.7043731880187987\n",
      "109 train_loss:  2.7043732261657714\n",
      "110 train_loss:  2.704373149871826\n",
      "111 train_loss:  2.704373207092285\n",
      "112 train_loss:  2.704373140335083\n",
      "113 train_loss:  2.7043731784820557\n",
      "114 train_loss:  2.7043732833862304\n",
      "115 train_loss:  2.7043732261657714\n",
      "116 train_loss:  2.7043731117248537\n",
      "117 train_loss:  2.7043731880187987\n",
      "118 train_loss:  2.7043731307983396\n",
      "119 train_loss:  2.704373207092285\n",
      "119 test_loss:  4.484903335571289\n",
      "120 train_loss:  2.7043732452392577\n",
      "121 train_loss:  2.7043731689453123\n",
      "122 train_loss:  2.704373149871826\n",
      "123 train_loss:  2.7043732452392577\n",
      "124 train_loss:  2.7043730926513674\n",
      "125 train_loss:  2.7043731880187987\n",
      "126 train_loss:  2.704373149871826\n",
      "127 train_loss:  2.704373207092285\n",
      "128 train_loss:  2.7043731689453123\n",
      "129 train_loss:  2.7043731117248537\n",
      "130 train_loss:  2.7043732452392577\n",
      "131 train_loss:  2.704373197555542\n",
      "132 train_loss:  2.7043731880187987\n",
      "133 train_loss:  2.7043732738494874\n",
      "134 train_loss:  2.7043732452392577\n",
      "135 train_loss:  2.704373264312744\n",
      "136 train_loss:  2.7043732261657714\n",
      "137 train_loss:  2.7043731689453123\n",
      "138 train_loss:  2.7043731880187987\n",
      "139 train_loss:  2.7043731880187987\n",
      "139 test_loss:  4.484903554916382\n",
      "140 train_loss:  2.704373264312744\n",
      "141 train_loss:  2.7043731307983396\n",
      "142 train_loss:  2.7043731594085694\n",
      "143 train_loss:  2.7043731689453123\n",
      "144 train_loss:  2.704373073577881\n",
      "145 train_loss:  2.7043732452392577\n",
      "146 train_loss:  2.7043731689453123\n",
      "147 train_loss:  2.704373149871826\n",
      "148 train_loss:  2.7043732738494874\n",
      "149 train_loss:  2.7043730926513674\n",
      "150 train_loss:  2.704373207092285\n",
      "151 train_loss:  2.7043732833862304\n",
      "152 train_loss:  2.7043731117248537\n",
      "153 train_loss:  2.7043732452392577\n",
      "154 train_loss:  2.7043731689453123\n",
      "155 train_loss:  2.7043731594085694\n",
      "156 train_loss:  2.704373264312744\n",
      "157 train_loss:  2.704373207092285\n",
      "158 train_loss:  2.704373207092285\n",
      "159 train_loss:  2.7043732357025148\n",
      "159 test_loss:  4.484903545379638\n",
      "160 train_loss:  2.7043732261657714\n",
      "161 train_loss:  2.704373207092285\n",
      "162 train_loss:  2.7043731689453123\n",
      "163 train_loss:  2.70437331199646\n",
      "164 train_loss:  2.7043732833862304\n",
      "165 train_loss:  2.7043731784820557\n",
      "166 train_loss:  2.7043732261657714\n",
      "167 train_loss:  2.7043732261657714\n",
      "168 train_loss:  2.704373264312744\n",
      "169 train_loss:  2.7043732738494874\n",
      "170 train_loss:  2.7043732833862304\n",
      "171 train_loss:  2.7043732261657714\n",
      "172 train_loss:  2.704373207092285\n",
      "173 train_loss:  2.7043731880187987\n",
      "174 train_loss:  2.7043732261657714\n",
      "175 train_loss:  2.7043732833862304\n",
      "176 train_loss:  2.7043731880187987\n",
      "177 train_loss:  2.7043731880187987\n",
      "178 train_loss:  2.7043731117248537\n",
      "179 train_loss:  2.704373264312744\n",
      "179 test_loss:  4.484903545379638\n",
      "180 train_loss:  2.7043732738494874\n",
      "181 train_loss:  2.7043731880187987\n",
      "182 train_loss:  2.704373264312744\n",
      "183 train_loss:  2.7043731117248537\n",
      "184 train_loss:  2.704373254776001\n",
      "185 train_loss:  2.7043731880187987\n",
      "186 train_loss:  2.7043731880187987\n",
      "187 train_loss:  2.7043732261657714\n",
      "188 train_loss:  2.704373207092285\n",
      "189 train_loss:  2.7043731689453123\n",
      "190 train_loss:  2.7043732833862304\n",
      "191 train_loss:  2.7043731307983396\n",
      "192 train_loss:  2.7043732452392577\n",
      "193 train_loss:  2.7043731784820557\n",
      "194 train_loss:  2.7043731880187987\n",
      "195 train_loss:  2.704373149871826\n",
      "196 train_loss:  2.7043730926513674\n",
      "197 train_loss:  2.7043732452392577\n",
      "198 train_loss:  2.7043732452392577\n",
      "199 train_loss:  2.7043731880187987\n",
      "199 test_loss:  4.484903688430786\n",
      "200 train_loss:  2.7043732452392577\n",
      "201 train_loss:  2.7043732833862304\n",
      "202 train_loss:  2.7043732357025148\n",
      "203 train_loss:  2.7043732166290284\n",
      "204 train_loss:  2.704373207092285\n",
      "205 train_loss:  2.7043732261657714\n",
      "206 train_loss:  2.7043731689453123\n",
      "207 train_loss:  2.704373254776001\n",
      "208 train_loss:  2.704373197555542\n",
      "209 train_loss:  2.7043731880187987\n",
      "210 train_loss:  2.704373149871826\n",
      "211 train_loss:  2.704373083114624\n",
      "212 train_loss:  2.704373197555542\n",
      "213 train_loss:  2.704373149871826\n",
      "214 train_loss:  2.7043732357025148\n",
      "215 train_loss:  2.7043732357025148\n",
      "216 train_loss:  2.7043732166290284\n",
      "217 train_loss:  2.7043731689453123\n",
      "218 train_loss:  2.704373264312744\n",
      "219 train_loss:  2.7043731212615967\n",
      "219 test_loss:  4.484903640747071\n",
      "220 train_loss:  2.7043731689453123\n",
      "221 train_loss:  2.7043731880187987\n",
      "222 train_loss:  2.704373207092285\n",
      "223 train_loss:  2.7043733024597167\n",
      "224 train_loss:  2.7043732261657714\n",
      "225 train_loss:  2.704373207092285\n",
      "226 train_loss:  2.704373207092285\n",
      "227 train_loss:  2.704373264312744\n",
      "228 train_loss:  2.7043731307983396\n",
      "229 train_loss:  2.7043731880187987\n",
      "230 train_loss:  2.704373149871826\n",
      "231 train_loss:  2.704373207092285\n",
      "232 train_loss:  2.7043731689453123\n",
      "233 train_loss:  2.7043731880187987\n",
      "234 train_loss:  2.7043732261657714\n",
      "235 train_loss:  2.7043730926513674\n",
      "236 train_loss:  2.7043731689453123\n",
      "237 train_loss:  2.704373197555542\n",
      "238 train_loss:  2.704373321533203\n",
      "239 train_loss:  2.7043732452392577\n",
      "239 test_loss:  4.4849034118652344\n",
      "240 train_loss:  2.704373359680176\n",
      "241 train_loss:  2.7043731880187987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 train_loss:  2.7043731594085694\n",
      "243 train_loss:  2.7043731307983396\n",
      "244 train_loss:  2.7043731689453123\n",
      "245 train_loss:  2.704373264312744\n",
      "246 train_loss:  2.7043732833862304\n",
      "247 train_loss:  2.704373207092285\n",
      "248 train_loss:  2.7043731689453123\n",
      "249 train_loss:  2.7043731880187987\n",
      "250 train_loss:  2.704373207092285\n",
      "251 train_loss:  2.7043731117248537\n",
      "252 train_loss:  2.7043731784820557\n",
      "253 train_loss:  2.7043731689453123\n",
      "254 train_loss:  2.704373149871826\n",
      "255 train_loss:  2.704373207092285\n",
      "256 train_loss:  2.704373149871826\n",
      "257 train_loss:  2.7043731117248537\n",
      "258 train_loss:  2.704373264312744\n",
      "259 train_loss:  2.704373149871826\n",
      "259 test_loss:  4.484903469085693\n",
      "260 train_loss:  2.7043732833862304\n",
      "261 train_loss:  2.704373149871826\n",
      "262 train_loss:  2.704373254776001\n",
      "263 train_loss:  2.7043733024597167\n",
      "264 train_loss:  2.704373140335083\n",
      "265 train_loss:  2.704373207092285\n",
      "266 train_loss:  2.704373207092285\n",
      "267 train_loss:  2.704373149871826\n",
      "268 train_loss:  2.704373149871826\n",
      "269 train_loss:  2.704373197555542\n",
      "270 train_loss:  2.7043731689453123\n",
      "271 train_loss:  2.7043731689453123\n",
      "272 train_loss:  2.70437331199646\n",
      "273 train_loss:  2.7043731307983396\n",
      "274 train_loss:  2.704373140335083\n",
      "275 train_loss:  2.7043731880187987\n",
      "276 train_loss:  2.7043731307983396\n",
      "277 train_loss:  2.7043732452392577\n",
      "278 train_loss:  2.7043731784820557\n",
      "279 train_loss:  2.7043732738494874\n",
      "279 test_loss:  4.484903621673584\n",
      "280 train_loss:  2.7043731117248537\n",
      "281 train_loss:  2.704373197555542\n",
      "282 train_loss:  2.7043731689453123\n",
      "283 train_loss:  2.704373264312744\n",
      "284 train_loss:  2.704373149871826\n",
      "285 train_loss:  2.7043731784820557\n",
      "286 train_loss:  2.704373149871826\n",
      "287 train_loss:  2.7043731307983396\n",
      "288 train_loss:  2.7043732833862304\n",
      "289 train_loss:  2.704373140335083\n",
      "290 train_loss:  2.704373149871826\n",
      "291 train_loss:  2.7043731880187987\n",
      "292 train_loss:  2.7043731117248537\n",
      "293 train_loss:  2.7043733024597167\n",
      "294 train_loss:  2.7043731689453123\n",
      "295 train_loss:  2.7043731689453123\n",
      "296 train_loss:  2.7043731880187987\n",
      "297 train_loss:  2.7043733024597167\n",
      "298 train_loss:  2.7043732261657714\n",
      "299 train_loss:  2.7043731117248537\n",
      "299 test_loss:  4.484903736114502\n",
      "300 train_loss:  2.7043731307983396\n",
      "301 train_loss:  2.7043731594085694\n",
      "302 train_loss:  2.7043731689453123\n",
      "303 train_loss:  2.704373149871826\n",
      "304 train_loss:  2.7043731689453123\n",
      "305 train_loss:  2.7043732452392577\n",
      "306 train_loss:  2.7043732452392577\n",
      "307 train_loss:  2.704373321533203\n",
      "308 train_loss:  2.704373149871826\n",
      "309 train_loss:  2.704373207092285\n",
      "310 train_loss:  2.704373140335083\n",
      "311 train_loss:  2.7043731880187987\n",
      "312 train_loss:  2.7043733024597167\n",
      "313 train_loss:  2.704373254776001\n",
      "314 train_loss:  2.704373149871826\n",
      "315 train_loss:  2.704373264312744\n",
      "316 train_loss:  2.7043731307983396\n",
      "317 train_loss:  2.7043732452392577\n",
      "318 train_loss:  2.704373207092285\n",
      "319 train_loss:  2.7043732452392577\n",
      "319 test_loss:  4.484903678894043\n",
      "320 train_loss:  2.7043732833862304\n",
      "321 train_loss:  2.7043732833862304\n",
      "322 train_loss:  2.7043732166290284\n",
      "323 train_loss:  2.7043731307983396\n",
      "324 train_loss:  2.7043731880187987\n",
      "325 train_loss:  2.7043732261657714\n",
      "326 train_loss:  2.7043731307983396\n",
      "327 train_loss:  2.704373207092285\n",
      "328 train_loss:  2.704373207092285\n",
      "329 train_loss:  2.7043731689453123\n",
      "330 train_loss:  2.7043732357025148\n",
      "331 train_loss:  2.7043731880187987\n",
      "332 train_loss:  2.704373149871826\n",
      "333 train_loss:  2.7043731880187987\n",
      "334 train_loss:  2.7043732357025148\n",
      "335 train_loss:  2.7043732452392577\n",
      "336 train_loss:  2.7043732166290284\n",
      "337 train_loss:  2.704373292922974\n",
      "338 train_loss:  2.7043733024597167\n",
      "339 train_loss:  2.704373207092285\n",
      "339 test_loss:  4.48490351676941\n",
      "340 train_loss:  2.704373207092285\n",
      "341 train_loss:  2.7043731307983396\n",
      "342 train_loss:  2.7043731784820557\n",
      "343 train_loss:  2.704373207092285\n",
      "344 train_loss:  2.7043731117248537\n",
      "345 train_loss:  2.7043733024597167\n",
      "346 train_loss:  2.704373149871826\n",
      "347 train_loss:  2.7043732261657714\n",
      "348 train_loss:  2.7043731689453123\n",
      "349 train_loss:  2.7043732452392577\n",
      "350 train_loss:  2.7043732452392577\n",
      "351 train_loss:  2.7043731117248537\n",
      "352 train_loss:  2.7043732261657714\n",
      "353 train_loss:  2.7043731307983396\n",
      "354 train_loss:  2.7043732452392577\n",
      "355 train_loss:  2.704373149871826\n",
      "356 train_loss:  2.7043731307983396\n",
      "357 train_loss:  2.7043732261657714\n",
      "358 train_loss:  2.7043733024597167\n",
      "359 train_loss:  2.7043731307983396\n",
      "359 test_loss:  4.484903621673584\n",
      "360 train_loss:  2.704373207092285\n",
      "361 train_loss:  2.7043732452392577\n",
      "362 train_loss:  2.7043730926513674\n",
      "363 train_loss:  2.7043732261657714\n",
      "364 train_loss:  2.7043732261657714\n",
      "365 train_loss:  2.7043731784820557\n",
      "366 train_loss:  2.7043732357025148\n",
      "367 train_loss:  2.7043732166290284\n",
      "368 train_loss:  2.704373073577881\n",
      "369 train_loss:  2.704373197555542\n",
      "370 train_loss:  2.704373207092285\n",
      "371 train_loss:  2.704373083114624\n",
      "372 train_loss:  2.7043731307983396\n",
      "373 train_loss:  2.7043731307983396\n",
      "374 train_loss:  2.7043732261657714\n",
      "375 train_loss:  2.7043731880187987\n",
      "376 train_loss:  2.704373207092285\n",
      "377 train_loss:  2.704373207092285\n",
      "378 train_loss:  2.7043732261657714\n",
      "379 train_loss:  2.7043731880187987\n",
      "379 test_loss:  4.484903583526611\n",
      "380 train_loss:  2.7043732452392577\n",
      "381 train_loss:  2.7043732452392577\n",
      "382 train_loss:  2.704373207092285\n",
      "383 train_loss:  2.7043731880187987\n",
      "384 train_loss:  2.7043731307983396\n",
      "385 train_loss:  2.7043731212615967\n",
      "386 train_loss:  2.7043731689453123\n",
      "387 train_loss:  2.704373207092285\n",
      "388 train_loss:  2.7043731880187987\n",
      "389 train_loss:  2.704373197555542\n",
      "390 train_loss:  2.7043731689453123\n",
      "391 train_loss:  2.7043732261657714\n",
      "392 train_loss:  2.704373207092285\n",
      "393 train_loss:  2.704373149871826\n",
      "394 train_loss:  2.704373264312744\n",
      "395 train_loss:  2.7043732166290284\n",
      "396 train_loss:  2.7043731117248537\n",
      "397 train_loss:  2.7043732261657714\n",
      "398 train_loss:  2.7043731689453123\n",
      "399 train_loss:  2.7043730926513674\n",
      "399 test_loss:  4.4849035263061525\n"
     ]
    }
   ],
   "source": [
    "input_num = 20\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_num, output_num, bias=False)\n",
    ")\n",
    "train_loader = loadArray(poly_features[:n_train, :input_num], labels[:n_train], batch_size)\n",
    "test_loader = loadArray(poly_features[n_train:, :input_num], labels[n_train:], batch_size)\n",
    "\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    train_loss, _ = train(net, train_loader, loss, optimizer)\n",
    "    print(epoch, \"train_loss: \", train_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        test_loss, _ = evaluate_test(net, test_loader, loss)\n",
    "        print(epoch, \"test_loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6a4767a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1078, -0.2056,  0.1194, -0.1864,  0.2000, -0.0049, -0.0409, -0.0948,\n",
       "         -0.0329,  0.1334,  0.1239,  0.1186, -0.0459,  0.0271,  0.1342,  0.1500,\n",
       "          0.0598,  0.0550, -0.2014, -0.0969]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f73f7",
   "metadata": {},
   "source": [
    "## L2正则化（防止过拟合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7a8885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yan\\AppData\\Local\\Temp\\ipykernel_23992\\1338213009.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_w, features, poly_features, labels = [torch.tensor(x, dtype=torch.float32) for x in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train_loss:  2.167507381439209\n",
      "1 train_loss:  1.5838525009155273\n",
      "2 train_loss:  1.2151785230636596\n",
      "3 train_loss:  0.9459301042556763\n",
      "4 train_loss:  0.7643800687789917\n",
      "5 train_loss:  0.6425672388076782\n",
      "6 train_loss:  0.548407654762268\n",
      "7 train_loss:  0.48396779417991637\n",
      "8 train_loss:  0.421153427362442\n",
      "9 train_loss:  0.3790281975269318\n",
      "10 train_loss:  0.3427979850769043\n",
      "11 train_loss:  0.312842390537262\n",
      "12 train_loss:  0.2872386109828949\n",
      "13 train_loss:  0.2648416483402252\n",
      "14 train_loss:  0.24570578455924988\n",
      "15 train_loss:  0.22741203308105468\n",
      "16 train_loss:  0.21196664810180665\n",
      "17 train_loss:  0.19772797346115112\n",
      "18 train_loss:  0.18490621268749238\n",
      "19 train_loss:  0.172829470038414\n",
      "19 test_loss:  0.6325866460800171\n",
      "20 train_loss:  0.16261752545833588\n",
      "21 train_loss:  0.1529921406507492\n",
      "22 train_loss:  0.14510275542736054\n",
      "23 train_loss:  0.13654311060905455\n",
      "24 train_loss:  0.1293346905708313\n",
      "25 train_loss:  0.12301763772964477\n",
      "26 train_loss:  0.11643536567687988\n",
      "27 train_loss:  0.11076606512069702\n",
      "28 train_loss:  0.10564541697502136\n",
      "29 train_loss:  0.1009058853983879\n",
      "30 train_loss:  0.09693452835083008\n",
      "31 train_loss:  0.09219888120889663\n",
      "32 train_loss:  0.08840117633342742\n",
      "33 train_loss:  0.08527668476104737\n",
      "34 train_loss:  0.082003732919693\n",
      "35 train_loss:  0.0786184823513031\n",
      "36 train_loss:  0.07617353409528732\n",
      "37 train_loss:  0.07371224850416183\n",
      "38 train_loss:  0.07108031898736954\n",
      "39 train_loss:  0.06878687351942063\n",
      "39 test_loss:  0.7673662841320038\n",
      "40 train_loss:  0.0661838161945343\n",
      "41 train_loss:  0.0642426297068596\n",
      "42 train_loss:  0.06241403728723526\n",
      "43 train_loss:  0.060566248446702955\n",
      "44 train_loss:  0.05899063467979431\n",
      "45 train_loss:  0.05759136706590653\n",
      "46 train_loss:  0.05575442671775818\n",
      "47 train_loss:  0.054102723896503446\n",
      "48 train_loss:  0.052920534759759906\n",
      "49 train_loss:  0.05196395665407181\n",
      "50 train_loss:  0.05033352851867676\n",
      "51 train_loss:  0.0491767030954361\n",
      "52 train_loss:  0.04799848333001137\n",
      "53 train_loss:  0.046849170327186586\n",
      "54 train_loss:  0.045647884756326675\n",
      "55 train_loss:  0.044664869755506514\n",
      "56 train_loss:  0.04368619278073311\n",
      "57 train_loss:  0.042644917219877246\n",
      "58 train_loss:  0.041844251155853274\n",
      "59 train_loss:  0.04079112008213997\n",
      "59 test_loss:  0.6674760493636132\n",
      "60 train_loss:  0.039978408813476564\n",
      "61 train_loss:  0.03927385374903679\n",
      "62 train_loss:  0.03839531093835831\n",
      "63 train_loss:  0.03763244435191154\n",
      "64 train_loss:  0.036933184266090394\n",
      "65 train_loss:  0.03602895468473435\n",
      "66 train_loss:  0.035247659385204314\n",
      "67 train_loss:  0.03456835061311722\n",
      "68 train_loss:  0.034075941741466526\n",
      "69 train_loss:  0.03329530961811542\n",
      "70 train_loss:  0.03282583892345428\n",
      "71 train_loss:  0.032072176784276964\n",
      "72 train_loss:  0.031357604265213015\n",
      "73 train_loss:  0.030782407224178313\n",
      "74 train_loss:  0.03026675321161747\n",
      "75 train_loss:  0.029575086534023284\n",
      "76 train_loss:  0.0291415311396122\n",
      "77 train_loss:  0.0286176860332489\n",
      "78 train_loss:  0.028064092621207237\n",
      "79 train_loss:  0.027581648379564287\n",
      "79 test_loss:  0.5065089363604784\n",
      "80 train_loss:  0.027083297073841096\n",
      "81 train_loss:  0.026499878987669943\n",
      "82 train_loss:  0.026014554798603057\n",
      "83 train_loss:  0.025645612627267837\n",
      "84 train_loss:  0.025004164278507233\n",
      "85 train_loss:  0.02466283440589905\n",
      "86 train_loss:  0.024139715582132338\n",
      "87 train_loss:  0.023863384798169136\n",
      "88 train_loss:  0.023360363394021987\n",
      "89 train_loss:  0.02281721830368042\n",
      "90 train_loss:  0.022437555789947508\n",
      "91 train_loss:  0.02202462114393711\n",
      "92 train_loss:  0.02169907309114933\n",
      "93 train_loss:  0.0213214623183012\n",
      "94 train_loss:  0.02090815782546997\n",
      "95 train_loss:  0.020604344233870506\n",
      "96 train_loss:  0.02012809470295906\n",
      "97 train_loss:  0.01986578069627285\n",
      "98 train_loss:  0.019497729763388635\n",
      "99 train_loss:  0.01912462688982487\n",
      "99 test_loss:  0.3624643522500992\n",
      "100 train_loss:  0.01875635676085949\n",
      "101 train_loss:  0.01844354599714279\n",
      "102 train_loss:  0.0181462824344635\n",
      "103 train_loss:  0.017866356633603572\n",
      "104 train_loss:  0.017551805675029754\n",
      "105 train_loss:  0.01724116154015064\n",
      "106 train_loss:  0.017041836902499198\n",
      "107 train_loss:  0.016674504205584525\n",
      "108 train_loss:  0.016332031637430192\n",
      "109 train_loss:  0.016138366796076296\n",
      "110 train_loss:  0.015782274454832077\n",
      "111 train_loss:  0.015506296642124653\n",
      "112 train_loss:  0.015260694548487663\n",
      "113 train_loss:  0.014966516867280006\n",
      "114 train_loss:  0.014726590663194657\n",
      "115 train_loss:  0.01459363840520382\n",
      "116 train_loss:  0.01423257276415825\n",
      "117 train_loss:  0.014129086136817933\n",
      "118 train_loss:  0.013764669522643089\n",
      "119 train_loss:  0.013568529039621352\n",
      "119 test_loss:  0.2542854629456997\n",
      "120 train_loss:  0.013346729055047035\n",
      "121 train_loss:  0.013127396479249001\n",
      "122 train_loss:  0.012886394821107388\n",
      "123 train_loss:  0.012655759863555431\n",
      "124 train_loss:  0.012458163052797318\n",
      "125 train_loss:  0.012241045795381069\n",
      "126 train_loss:  0.012166460454463958\n",
      "127 train_loss:  0.011914347410202026\n",
      "128 train_loss:  0.011781074926257134\n",
      "129 train_loss:  0.011450506150722503\n",
      "130 train_loss:  0.011315247975289822\n",
      "131 train_loss:  0.011073664911091328\n",
      "132 train_loss:  0.01092143051326275\n",
      "133 train_loss:  0.010783513896167279\n",
      "134 train_loss:  0.010612747333943844\n",
      "135 train_loss:  0.01042410921305418\n",
      "136 train_loss:  0.010209167934954166\n",
      "137 train_loss:  0.01006741613149643\n",
      "138 train_loss:  0.009982685670256614\n",
      "139 train_loss:  0.009763545915484428\n",
      "139 test_loss:  0.17696330323815346\n",
      "140 train_loss:  0.009664244800806045\n",
      "141 train_loss:  0.009447004720568657\n",
      "142 train_loss:  0.009319995157420635\n",
      "143 train_loss:  0.009176549017429352\n",
      "144 train_loss:  0.009057579971849918\n",
      "145 train_loss:  0.008922870494425297\n",
      "146 train_loss:  0.00876445583999157\n",
      "147 train_loss:  0.008613345064222813\n",
      "148 train_loss:  0.00848008893430233\n",
      "149 train_loss:  0.00837737426161766\n",
      "150 train_loss:  0.008245443403720855\n",
      "151 train_loss:  0.008103898018598557\n",
      "152 train_loss:  0.007999438345432281\n",
      "153 train_loss:  0.007886410858482122\n",
      "154 train_loss:  0.007727463077753782\n",
      "155 train_loss:  0.007686122730374337\n",
      "156 train_loss:  0.007500259755179286\n",
      "157 train_loss:  0.007410562932491303\n",
      "158 train_loss:  0.007284718137234449\n",
      "159 train_loss:  0.007196294032037258\n",
      "159 test_loss:  0.12315203372389077\n",
      "160 train_loss:  0.0071190398186445234\n",
      "161 train_loss:  0.006997210718691349\n",
      "162 train_loss:  0.006873569749295712\n",
      "163 train_loss:  0.006778072901070118\n",
      "164 train_loss:  0.006696429066359997\n",
      "165 train_loss:  0.006602553315460682\n",
      "166 train_loss:  0.006559358928352594\n",
      "167 train_loss:  0.006416740901768208\n",
      "168 train_loss:  0.006315864995121956\n",
      "169 train_loss:  0.0062683261185884475\n",
      "170 train_loss:  0.006159316971898079\n",
      "171 train_loss:  0.006074502058327198\n",
      "172 train_loss:  0.005976311224512756\n",
      "173 train_loss:  0.005930629596114159\n",
      "174 train_loss:  0.005830165632069111\n",
      "175 train_loss:  0.005752229560166597\n",
      "176 train_loss:  0.00566588880494237\n",
      "177 train_loss:  0.005605437904596329\n",
      "178 train_loss:  0.005544813610613346\n",
      "179 train_loss:  0.005439947880804538\n",
      "179 test_loss:  0.08483047007583082\n",
      "180 train_loss:  0.005375735592097044\n",
      "181 train_loss:  0.005318576321005821\n",
      "182 train_loss:  0.005241356287151575\n",
      "183 train_loss:  0.0051727830991148945\n",
      "184 train_loss:  0.005111566912382841\n",
      "185 train_loss:  0.005054291225969791\n",
      "186 train_loss:  0.004983303174376488\n",
      "187 train_loss:  0.00492414491251111\n",
      "188 train_loss:  0.004859855957329273\n",
      "189 train_loss:  0.004812775840982795\n",
      "190 train_loss:  0.00474257118999958\n",
      "191 train_loss:  0.0046740049868822095\n",
      "192 train_loss:  0.004619571715593338\n",
      "193 train_loss:  0.004572280216962099\n",
      "194 train_loss:  0.004511597920209169\n",
      "195 train_loss:  0.004460755698382854\n",
      "196 train_loss:  0.004411593284457922\n",
      "197 train_loss:  0.004361320622265339\n",
      "198 train_loss:  0.004314394425600767\n",
      "199 train_loss:  0.004256414882838726\n",
      "199 test_loss:  0.05844309005886316\n",
      "200 train_loss:  0.0042205115407705305\n",
      "201 train_loss:  0.004157024212181568\n",
      "202 train_loss:  0.004116875417530537\n",
      "203 train_loss:  0.00408261370845139\n",
      "204 train_loss:  0.004026677478104829\n",
      "205 train_loss:  0.0039771397784352305\n",
      "206 train_loss:  0.003956236951053143\n",
      "207 train_loss:  0.0038997985795140265\n",
      "208 train_loss:  0.0038544285483658315\n",
      "209 train_loss:  0.0038198708556592465\n",
      "210 train_loss:  0.003789988961070776\n",
      "211 train_loss:  0.0037340584211051466\n",
      "212 train_loss:  0.0037239013239741326\n",
      "213 train_loss:  0.003654628600925207\n",
      "214 train_loss:  0.003609951063990593\n",
      "215 train_loss:  0.0035772944986820223\n",
      "216 train_loss:  0.0035563204996287823\n",
      "217 train_loss:  0.003505974058061838\n",
      "218 train_loss:  0.0034716601204127073\n",
      "219 train_loss:  0.003460828997194767\n",
      "219 test_loss:  0.04035770984366536\n",
      "220 train_loss:  0.00340378375723958\n",
      "221 train_loss:  0.0033824832364916803\n",
      "222 train_loss:  0.0033361696358770133\n",
      "223 train_loss:  0.0033131752163171766\n",
      "224 train_loss:  0.0032816296443343163\n",
      "225 train_loss:  0.003260356318205595\n",
      "226 train_loss:  0.003227653652429581\n",
      "227 train_loss:  0.0031887872889637945\n",
      "228 train_loss:  0.003167953919619322\n",
      "229 train_loss:  0.0031427770853042604\n",
      "230 train_loss:  0.0031147617939859627\n",
      "231 train_loss:  0.0030762909166514875\n",
      "232 train_loss:  0.0030641854368150235\n",
      "233 train_loss:  0.0030381577648222447\n",
      "234 train_loss:  0.003002108735963702\n",
      "235 train_loss:  0.0029838824830949306\n",
      "236 train_loss:  0.0029522191639989614\n",
      "237 train_loss:  0.002923965947702527\n",
      "238 train_loss:  0.002904931791126728\n",
      "239 train_loss:  0.002886073235422373\n",
      "239 test_loss:  0.027461514994502067\n",
      "240 train_loss:  0.0028615607880055904\n",
      "241 train_loss:  0.0028354900702834127\n",
      "242 train_loss:  0.0028113266732543705\n",
      "243 train_loss:  0.002798677096143365\n",
      "244 train_loss:  0.002773156873881817\n",
      "245 train_loss:  0.002747643431648612\n",
      "246 train_loss:  0.002729877745732665\n",
      "247 train_loss:  0.002723061162978411\n",
      "248 train_loss:  0.0026992686372250317\n",
      "249 train_loss:  0.002667613010853529\n",
      "250 train_loss:  0.0026467487961053847\n",
      "251 train_loss:  0.0026259624399244787\n",
      "252 train_loss:  0.002625574618577957\n",
      "253 train_loss:  0.002590938098728657\n",
      "254 train_loss:  0.0025746994372457268\n",
      "255 train_loss:  0.002560193734243512\n",
      "256 train_loss:  0.0025473521277308464\n",
      "257 train_loss:  0.0025230223312973977\n",
      "258 train_loss:  0.002507455898448825\n",
      "259 train_loss:  0.0024918145313858986\n",
      "259 test_loss:  0.018769574887119233\n",
      "260 train_loss:  0.0024814919009804727\n",
      "261 train_loss:  0.0024532231874763964\n",
      "262 train_loss:  0.002455410799011588\n",
      "263 train_loss:  0.0024362406227737667\n",
      "264 train_loss:  0.0024095992092043163\n",
      "265 train_loss:  0.002397976256906986\n",
      "266 train_loss:  0.0023828190285712482\n",
      "267 train_loss:  0.00237304980866611\n",
      "268 train_loss:  0.0023561979178339243\n",
      "269 train_loss:  0.0023511734139174223\n",
      "270 train_loss:  0.0023353157471865415\n",
      "271 train_loss:  0.0023138016555458306\n",
      "272 train_loss:  0.002308021504431963\n",
      "273 train_loss:  0.0022928966674953697\n",
      "274 train_loss:  0.0022788070514798164\n",
      "275 train_loss:  0.002263910248875618\n",
      "276 train_loss:  0.002257483284920454\n",
      "277 train_loss:  0.002248191833496094\n",
      "278 train_loss:  0.0022303750272840263\n",
      "279 train_loss:  0.002231092965230346\n",
      "279 test_loss:  0.01265730082988739\n",
      "280 train_loss:  0.0022026851773262026\n",
      "281 train_loss:  0.002196261612698436\n",
      "282 train_loss:  0.002185168517753482\n",
      "283 train_loss:  0.002173073594458401\n",
      "284 train_loss:  0.002164699528366327\n",
      "285 train_loss:  0.002154049817472696\n",
      "286 train_loss:  0.0021434700768440964\n",
      "287 train_loss:  0.00213797502219677\n",
      "288 train_loss:  0.00212237436324358\n",
      "289 train_loss:  0.002109886948019266\n",
      "290 train_loss:  0.00210351774469018\n",
      "291 train_loss:  0.0020971595402807\n",
      "292 train_loss:  0.0020881742518395184\n",
      "293 train_loss:  0.0020854959078133108\n",
      "294 train_loss:  0.0020627985429018734\n",
      "295 train_loss:  0.0020544334221631287\n",
      "296 train_loss:  0.002052584020420909\n",
      "297 train_loss:  0.002041364349424839\n",
      "298 train_loss:  0.0020329576544463636\n",
      "299 train_loss:  0.0020255332347005606\n",
      "299 test_loss:  0.008609199963975698\n",
      "300 train_loss:  0.0020173710491508245\n",
      "301 train_loss:  0.002006313353776932\n",
      "302 train_loss:  0.0020006776601076124\n",
      "303 train_loss:  0.0019968617893755436\n",
      "304 train_loss:  0.0019795840559527276\n",
      "305 train_loss:  0.001979448115453124\n",
      "306 train_loss:  0.0019779941299930214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 train_loss:  0.001960717895999551\n",
      "308 train_loss:  0.0019567079190164803\n",
      "309 train_loss:  0.0019555699266493322\n",
      "310 train_loss:  0.0019370961841195821\n",
      "311 train_loss:  0.0019370533525943756\n",
      "312 train_loss:  0.0019266691850498318\n",
      "313 train_loss:  0.0019257957488298416\n",
      "314 train_loss:  0.001919765411876142\n",
      "315 train_loss:  0.0019103243201971055\n",
      "316 train_loss:  0.001899999240413308\n",
      "317 train_loss:  0.0018994388729333877\n",
      "318 train_loss:  0.0018917240109294652\n",
      "319 train_loss:  0.0018874911312013864\n",
      "319 test_loss:  0.00577513222116977\n",
      "320 train_loss:  0.00188315499573946\n",
      "321 train_loss:  0.0018710606638342143\n",
      "322 train_loss:  0.0018799310084432364\n",
      "323 train_loss:  0.001863116854801774\n",
      "324 train_loss:  0.001858607642352581\n",
      "325 train_loss:  0.001847792873159051\n",
      "326 train_loss:  0.0018424732610583306\n",
      "327 train_loss:  0.001844533197581768\n",
      "328 train_loss:  0.0018308104760944844\n",
      "329 train_loss:  0.0018290576431900262\n",
      "330 train_loss:  0.0018215703312307596\n",
      "331 train_loss:  0.0018200434744358063\n",
      "332 train_loss:  0.001815170906484127\n",
      "333 train_loss:  0.0018059220910072327\n",
      "334 train_loss:  0.0018025608034804464\n",
      "335 train_loss:  0.0018035295745357871\n",
      "336 train_loss:  0.0017956343898549677\n",
      "337 train_loss:  0.001787891834974289\n",
      "338 train_loss:  0.001789866778999567\n",
      "339 train_loss:  0.0017809958662837744\n",
      "339 test_loss:  0.003993487115949392\n",
      "340 train_loss:  0.0017820429895073176\n",
      "341 train_loss:  0.0017724736407399178\n",
      "342 train_loss:  0.0017672309279441834\n",
      "343 train_loss:  0.0017615800257772206\n",
      "344 train_loss:  0.0017618936859071254\n",
      "345 train_loss:  0.0017567178886383772\n",
      "346 train_loss:  0.001748435804620385\n",
      "347 train_loss:  0.0017448213044553996\n",
      "348 train_loss:  0.0017485262220725418\n",
      "349 train_loss:  0.0017382795084267854\n",
      "350 train_loss:  0.0017311204364523292\n",
      "351 train_loss:  0.0017296810913830995\n",
      "352 train_loss:  0.001727914083749056\n",
      "353 train_loss:  0.001720112543553114\n",
      "354 train_loss:  0.0017209309712052346\n",
      "355 train_loss:  0.0017168195452541112\n",
      "356 train_loss:  0.001718417084775865\n",
      "357 train_loss:  0.0017124934121966362\n",
      "358 train_loss:  0.0017011163756251336\n",
      "359 train_loss:  0.0017005445901304484\n",
      "359 test_loss:  0.002809928837232292\n",
      "360 train_loss:  0.0016949552623555064\n",
      "361 train_loss:  0.001695023411884904\n",
      "362 train_loss:  0.0016958533320575952\n",
      "363 train_loss:  0.0016874977480620145\n",
      "364 train_loss:  0.0016869138367474079\n",
      "365 train_loss:  0.0016782866418361664\n",
      "366 train_loss:  0.0016760177165269852\n",
      "367 train_loss:  0.0016727295704185962\n",
      "368 train_loss:  0.0016738691460341216\n",
      "369 train_loss:  0.001675297487527132\n",
      "370 train_loss:  0.001669571790844202\n",
      "371 train_loss:  0.0016665403172373772\n",
      "372 train_loss:  0.0016656152717769146\n",
      "373 train_loss:  0.001663106707856059\n",
      "374 train_loss:  0.0016619102749973536\n",
      "375 train_loss:  0.0016550122108310462\n",
      "376 train_loss:  0.0016513901622965932\n",
      "377 train_loss:  0.0016468050144612788\n",
      "378 train_loss:  0.0016498200222849846\n",
      "379 train_loss:  0.0016392660327255726\n",
      "379 test_loss:  0.0021097891218960284\n",
      "380 train_loss:  0.0016464169230312109\n",
      "381 train_loss:  0.0016364819929003715\n",
      "382 train_loss:  0.0016332097072154284\n",
      "383 train_loss:  0.0016329224221408368\n",
      "384 train_loss:  0.0016230744402855634\n",
      "385 train_loss:  0.0016313495580106974\n",
      "386 train_loss:  0.0016254069749265909\n",
      "387 train_loss:  0.0016171812172979117\n",
      "388 train_loss:  0.0016182877216488123\n",
      "389 train_loss:  0.0016139426082372665\n",
      "390 train_loss:  0.0016113545559346676\n",
      "391 train_loss:  0.001609657658264041\n",
      "392 train_loss:  0.001610675361007452\n",
      "393 train_loss:  0.001603622892871499\n",
      "394 train_loss:  0.0016066895704716444\n",
      "395 train_loss:  0.0016016307938843965\n",
      "396 train_loss:  0.001598533373326063\n",
      "397 train_loss:  0.0015991326933726669\n",
      "398 train_loss:  0.0015929597662761807\n",
      "399 train_loss:  0.0015915632946416735\n",
      "399 test_loss:  0.0017664054874330759\n",
      "8.060469627380371\n",
      "tensor([[ 4.9642,  1.2985, -3.1504,  5.0644, -0.6913,  1.5220, -0.2690,  0.1897,\n",
      "         -0.1646, -0.1007, -0.0981, -0.1332, -0.1025, -0.1587, -0.0516,  0.1653,\n",
      "         -0.0368,  0.0289, -0.0788, -0.1640]])\n"
     ]
    }
   ],
   "source": [
    "# 在优化器中加入weight_decay参数，但是还不知道怎么确定weight_decay\n",
    "weight_decay = 1e-4\n",
    "input_num = 20\n",
    "output_num = 1\n",
    "\n",
    "# NumPy ndarray转换为tensor\n",
    "true_w, features, poly_features, labels = [torch.tensor(x, dtype=torch.float32) for x in\n",
    "                                           [true_w, features, poly_features, labels]]\n",
    "\n",
    "train_loader = loadArray(poly_features[:n_train, :input_num], labels[:n_train], batch_size)\n",
    "test_loader = loadArray(poly_features[n_train:, :input_num], labels[n_train:], batch_size)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_num, output_num, bias=False)\n",
    ")\n",
    "\n",
    "# 优化器中加入weight_decay参数\n",
    "lr = 0.01\n",
    "loss = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    train_loss, _ = train(net, train_loader, loss, optimizer)\n",
    "    print(epoch, \"train_loss: \", train_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        test_loss, _ = evaluate_test(net, test_loader, loss)\n",
    "        print(epoch, \"test_loss: \", test_loss)\n",
    "\n",
    "print(net[0].weight.norm().item())\n",
    "print(net[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb103444",
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
